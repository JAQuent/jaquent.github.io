<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Mixed linear models" />
<meta property="og:description" content="Preface This work documents my attempt to understand the model structure of mixed linear models or multilevel models. I adapted a formula that I found on Wikipedia and work myself through the different models and test whether I am able to retrieve the different parameters with lmerTest. Bodo Winter&rsquo;s tutorial on the lme was very useful for this. The main motivation for doing all this is to use basically model 5 in my analysis of my first experiment." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://jaquent.github.io/2018/03/mixed-linear-models/" /><meta property="article:published_time" content="2018-03-14T00:00:00&#43;00:00"/>
<meta property="article:modified_time" content="2018-03-14T00:00:00&#43;00:00"/>

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Mixed linear models"/>
<meta name="twitter:description" content="Preface This work documents my attempt to understand the model structure of mixed linear models or multilevel models. I adapted a formula that I found on Wikipedia and work myself through the different models and test whether I am able to retrieve the different parameters with lmerTest. Bodo Winter&rsquo;s tutorial on the lme was very useful for this. The main motivation for doing all this is to use basically model 5 in my analysis of my first experiment."/>



    <link rel="canonical" href="https://jaquent.github.io/2018/03/mixed-linear-models/">

    <title>
      
        Mixed linear models | Jörn Alexander Quent&#39;s notebook
      
    </title>

    
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <link href="https://jaquent.github.iocss/style.css" rel="stylesheet">

    

    

    
  </head>
  <body>
    
      <header class="blog-header">
    <nav class="navbar navbar-expand-md navbar-light bg-light">
        <a class="navbar-brand" href="/">
            Jörn Alexander Quent&#39;s notebook
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
            aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse justify-content-between" id="navbarNav">
            <ul class="navbar-nav">
                
                
                <li class="nav-item ">
                    
                        <a class="nav-link" href="/">Home</a>
                    
                </li>
                
                <li class="nav-item ">
                    
                        <a class="nav-link" href="/about/">About</a>
                    
                </li>
                
            </ul>
            
        </div>
    </nav>
</header>
    

    
    <div class="container">
      <div class="row">
        <div class="col-12 col-lg-8 blog-main">

          

<header>
    <h2 class="blog-post-title">
        <a class="text-dark" href="/2018/03/mixed-linear-models/">Mixed linear models</a>
    </h2>
    


<div class="blog-post-date text-secondary">
    
        Mar 14, 2018
    
    
        by <span rel="author">Alex Quent</span>
    
</div>

    
<div class="blog-post-tags text-secondary">
    <strong>Tags:</strong>
    
        <a class="badge badge-primary" href="/tags/schemavr">schemaVR</a>
    
        <a class="badge badge-primary" href="/tags/mixed-linear-models">mixed linear models</a>
    
        <a class="badge badge-primary" href="/tags/lme4/lmertest">lme4/lmerTest</a>
    
        <a class="badge badge-primary" href="/tags/rmarkdown">rmarkdown</a>
    
</div>

    
<div class="blog-post-categories text-secondary">
    <strong>Categories:</strong>
    
        <a class="badge badge-primary" href="/categories/general">general</a>
    
</div>

    <hr>
</header>
<article class="blog-post">
    

<h1 id="preface">Preface</h1>

<p>This work documents my attempt to understand the model structure of mixed linear models or multilevel models. I adapted a formula that I found on <a href="https://en.wikipedia.org/wiki/Multilevel_model">Wikipedia</a> and work myself through the different models and test whether I am able to retrieve the different parameters with <em>lmerTest</em>. Bodo Winter&rsquo;s <a href="http://www.bodowinter.com/tutorial/bw_LME_tutorial.pdf">tutorial</a> on the <em>lme</em> was very useful for this. The main motivation for doing all this is to use basically model 5 in my analysis of my first experiment. This work is also part of the process of pre-registering the study with a complete analysis script, which I will upload soon.</p>

<p>The design of the fictive experiment that 100 subjects see 20 objects and provide a continuous measure of memory for each objects. The expectancy of the objects (<em>X</em>) servers as a predictor for the memory performance (<em>Y</em>).</p>

<p>I start with very easy models and add more effects in order to finally arrive at my target model (model 5).</p>

<h2 id="libraries-functions">Libraries &amp; functions</h2>

<pre><code class="language-r">library(lmerTest)
library(ggplot2)

pValue &lt;-function(x, sign = '='){
  if (inherits(x, &quot;lm&quot;)){
    s &lt;- summary.lm(x)
    x &lt;- pf(s$fstatistic[1L], s$fstatistic[2L], s$fstatistic[3L], lower.tail = FALSE)
    if(x &gt; 1){
      stop(&quot;There is no p-value greater than 1&quot;)
    } else if(x &lt; 0.001){
      x.converted &lt;- '&lt; .001'
    } else{
      x.converted &lt;- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  } else {
    if(x &gt; 1){
      stop(&quot;There is no p-value greater than 1&quot;)
    } else if(x &lt; 0.001){
      x.converted &lt;- '&lt; .001'
    } else{
      x.converted &lt;- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  }
  return(x.converted)
}

rValue &lt;-function(x){
  if (inherits(x, &quot;lm&quot;)){
    r.squared &lt;- summary(x)$r.squared
    x.converted &lt;- paste('=',substr(as.character(round(r.squared, 3)), 2,5)) 
  } else {
    if (x &lt; 0){
      x.converted &lt;- paste('= -',substr(as.character(abs(round(x, 3))), 2,5), sep = '') 
    } else {
      x.converted &lt;- paste('=',substr(as.character(abs(round(x, 3))), 2,5)) 
    }
  }
  return(x.converted) 
}
</code></pre>

<h2 id="model-1-simple-regression-model-with-1-level">Model 1: Simple regression model with 1 level</h2>

<p>Assume that expectancy and memory performance was aggregated across subjects.In this model, objects&rsquo; memorability is predicted by their average expectancy values. The corresponding formula looks like this:
<em>Y</em><sub><em>i</em></sub> = <em>β</em><sub>0</sub> + <em>β</em><sub>1</sub><em>X</em><sub><em>i</em></sub> + <em>e</em><sub><em>i</em></sub></p>

<ul>
<li><em>Y</em><sub><em>i</em></sub> refers to memorability of object i.</li>
<li><em>X</em><sub><em>i</em></sub> refers to average expectancy of object i.</li>
<li><em>β</em><sub>0</sub> refers to the intercept.</li>
<li><em>β</em><sub>1</sub> refers to the linear slope.</li>
<li><em>e</em><sub><em>i</em></sub> refers to the random errors of prediction.</li>
</ul>

<pre><code class="language-r"># Generating data set
# General values and variables
numObj &lt;- 20
e      &lt;- rnorm(numObj, mean = 0, sd = 0.1)
x      &lt;- scale(runif(numObj, min = -100, max = 100))
y      &lt;- c()

# Coefficients
beta0  &lt;- 8
beta1  &lt;- 0.3

for(i in 1:numObj){
  y[i] &lt;- beta0 + beta1*x[i] + e[i]
}

dataFrame1 &lt;- data.frame(y = y, x = x, objNum = factor(1:20)) 

model1 &lt;- lm(y ~  x, data = dataFrame1)
summary(model1)
</code></pre>

<pre><code>## 
## Call:
## lm(formula = y ~ x, data = dataFrame1)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.20890 -0.04468  0.01759  0.05531  0.13807 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  7.98510    0.01975  404.24  &lt; 2e-16 ***
## x            0.29759    0.02027   14.68 1.84e-11 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 0.08834 on 18 degrees of freedom
## Multiple R-squared:  0.923,  Adjusted R-squared:  0.9187 
## F-statistic: 215.6 on 1 and 18 DF,  p-value: 1.84e-11
</code></pre>

<p>Unsurprisingly, <em>lm</em> returns estimates that are very close the the true values $\hat{\beta_{0}} =$ 7.9850955 (vs 8) and $\hat{\beta_{1}} =$ 0.297594 (vs 0.3).</p>

<h2 id="model-2-a-random-intercept-for-each-subject">Model 2: A random intercept for each subject</h2>

<p>Now, I don&rsquo;t aggregated anymore and there is one value for each subject and each object and I allow for different intercepts for each subject, which basically means that I assume that my subjects&rsquo; general memory performance varies.</p>

<p>Level 1:
<em>Y</em><sub>*i<strong>j*</sub> = <em>β</em><sub>0<em>j</em></sub> + <em>β</em><sub>1</sub><em>X</em><sub>*i</strong>j*</sub> + <em>e</em><sub>*i*<em>j</em></sub>
 Level 2:
<em>β</em><sub>0<em>j</em></sub> = <em>γ</em><sub>00</sub> + <em>γ</em><sub>01</sub><em>W</em><sub><em>j</em></sub> + <em>u</em><sub>0<em>j</em></sub></p>

<ul>
<li><em>Y</em><sub>*i*<em>j</em></sub> refers to memory score for object i and subject j.</li>
<li><em>β</em><sub>0<em>j</em></sub> refers to the intercept of the dependent variable in subject j (level 2) in other words the individual difference in general performance across subjects.</li>
<li><em>β</em><sub>1</sub> refers to the linear slope.</li>
<li><em>X</em><sub>*i*<em>j</em></sub> refers to the level 1 predictor.</li>
<li><em>e</em><sub>*i*<em>j</em></sub> refers to the random errors of prediction for the level 1 equation.</li>
<li><em>γ</em><sub>00</sub> refers to the overall intercept. This is the grand mean of the scores on the dependent variable across all the subjects when all the predictors are equal to 0.</li>
<li><em>γ</em><sub>01</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub><em>j</em></sub> refers to the level 2 predictor.</li>
<li><em>u</em><sub>0<em>j</em></sub> refers to the random error component for the deviation of the intercept of a subject from the overall intercept.</li>
</ul>

<pre><code class="language-r"># Generating data set
# General values and variables
numObj &lt;- 20
numSub &lt;- 100
e      &lt;- rnorm(numObj * numSub, mean = 0, sd = 0.1)
x      &lt;- scale(runif(numObj * numSub, min = -100, max = 100))
y      &lt;- c()
index  &lt;- 1

# Coefficients
gamma00 &lt;- 18
gamma01 &lt;- 0.5
beta1   &lt;- -100
w       &lt;- runif(numSub, min = -3, max = 3)
u0      &lt;- rnorm(numSub, mean = 0, sd = 0.1)
meanBeta0 &lt;- mean(gamma00 + gamma01*w + u0) # I should be able to retrieve that parameter. 

for(j in 1:numSub){
  for(i in 1:numObj){
    y[index] &lt;- gamma00 + gamma01*w[j]+ u0[j] + beta1*x[index] + e[index]
    index &lt;- index + 1
  } 
}

dataFrame2 &lt;- data.frame(y = y, x = x, subNo = factor(rep(1:numSub, each = numObj)), objNum = factor(rep(1:numObj, numSub))) 

model2 &lt;- lmer(y ~  x + 
                 (1 | subNo), data = dataFrame2)
summary(model2)
</code></pre>

<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite's method [
## lmerModLmerTest]
## Formula: y ~ x + (1 | subNo)
##    Data: dataFrame2
## 
## REML criterion at convergence: -2857.9
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.3190 -0.6732 -0.0093  0.6534  3.5334 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev.
##  subNo    (Intercept) 0.685895 0.8282  
##  Residual             0.009703 0.0985  
## Number of obs: 2000, groups:  subNo, 100
## 
## Fixed effects:
##               Estimate Std. Error         df t value Pr(&gt;|t|)    
## (Intercept)  1.806e+01  8.285e-02  9.900e+01     218   &lt;2e-16 ***
## x           -1.000e+02  2.253e-03  1.899e+03  -44394   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##   (Intr)
## x 0.000
</code></pre>

<pre><code class="language-r">anova(model2)
</code></pre>

<pre><code>## Type III Analysis of Variance Table with Satterthwaite's method
##     Sum Sq  Mean Sq NumDF  DenDF    F value    Pr(&gt;F)    
## x 19123395 19123395     1 1899.1 1970854622 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<pre><code class="language-r"># Extract intercept for each subject by coef(model2). 
</code></pre>

<p>As you can see above (fixed effects), $\hat{\beta_{0.}} =$ 18.0612368 is approximating the true (18.0601846). The same is true for $\hat{\beta_{1}} =$ -100.0019619 (vs -100). The estimated level 1 intercept for subject 1 was also close to the true value (18.8278538 vs 18.8320122).</p>

<h2 id="model-3-random-intercept-and-slope-for-each-subject">Model 3: Random intercept and slope for each subject</h2>

<p>In the next model, I am additionally allowing the slope randomly different for each subject. This means that the strength of the relationship between the level 1 predictor <em>X</em> and the memory scores <em>Y</em> varies across subjects.</p>

<p>Level 1:
<em>Y</em><sub>*i<strong>j*</sub> = <em>β</em><sub>0<em>j</em></sub> + <em>β</em><sub>1<em>j</em></sub><em>X</em><sub>*i</strong>j*</sub> + <em>e</em><sub>*i*<em>j</em></sub></p>

<p>Level 2:
<em>β</em><sub>0<em>j</em></sub> = <em>γ</em><sub>00</sub> + <em>γ</em><sub>01</sub><em>W</em><sub><em>j</em></sub> + <em>u</em><sub>0<em>j</em></sub>
<em>β</em><sub>1<em>j</em></sub> = <em>γ</em><sub>10</sub> + <em>u</em><sub>1<em>j</em></sub></p>

<ul>
<li><em>Y</em><sub>*i*<em>j</em></sub> refers to memory score for object i and subject j.</li>
<li><em>β</em><sub>0<em>j</em></sub> refers to the intercept of the dependent variable in subject j (level 2) in other words the individual difference in general performance across subjects.</li>
<li><em>β</em><sub>1<em>j</em></sub> refers to the linear slope that varies across subjects in other words the relationship <em>X</em> and <em>Y</em> is different across subjects.</li>
<li><em>X</em><sub>*i*<em>j</em></sub> refers to the level 1 predictor.</li>
<li><em>e</em><sub>*i*<em>j</em></sub> refers to the random errors of prediction for the level 1 equation.</li>
<li><em>γ</em><sub>00</sub> refers to the overall intercept. This is the grand mean of the scores on the dependent variable across all the subjects when all the predictors are equal to 0.</li>
<li><em>γ</em><sub>01</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub><em>j</em></sub> refers to the level 2 predictor.</li>
<li><em>u</em><sub>0<em>j</em></sub> refers to the random error component for the deviation of the intercept of a subject from the overall intercept.</li>
<li><em>γ</em><sub>10</sub> overall slope of the level 2.</li>
<li><em>u</em><sub>1<em>j</em></sub> refers to the random error component for the deviation of the slope of a subject from the overall intercept.</li>
</ul>

<pre><code class="language-r"># Generating data set
# General values and variables
numObj &lt;- 20
numSub &lt;- 100
e      &lt;- rnorm(numObj * numSub, mean = 0, sd = 0.1)
x      &lt;- scale(runif(numObj * numSub, min = -100, max = 100))
y      &lt;- c()
index  &lt;- 1

# Coefficients
gamma00 &lt;- 18
gamma01 &lt;- 0.5
gamma10 &lt;- -100
w       &lt;- runif(numSub, min = -3, max = 3)
u0      &lt;- rnorm(numSub, mean = 0, sd = 0.1)
u1      &lt;- rnorm(numSub, mean = 0, sd = 0.1)
meanBeta0 &lt;- mean(gamma00 + gamma01*w + u0) # I should be able to retrieve that parameter. 

for(j in 1:numSub){
  for(i in 1:numObj){
    y[index] &lt;- gamma00 + gamma01*w[j]+ u0[j] + (gamma10 + u1[j])*x[index] + e[index]
    index &lt;- index + 1
  } 
}

dataFrame3 &lt;- data.frame(y = y, x = x, subNo = factor(rep(1:numSub, each = numObj)), objNum = factor(rep(1:numObj, numSub))) 

model3 &lt;- lmer(y ~  x + 
                 (1 + x | subNo), data = dataFrame3)
summary(model3)
</code></pre>

<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite's method [
## lmerModLmerTest]
## Formula: y ~ x + (1 + x | subNo)
##    Data: dataFrame3
## 
## REML criterion at convergence: -2552
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -2.9327 -0.6122  0.0083  0.6447  3.2559 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr
##  subNo    (Intercept) 0.868034 0.93168      
##           x           0.008187 0.09048  0.11
##  Residual             0.009730 0.09864      
## Number of obs: 2000, groups:  subNo, 100
## 
## Fixed effects:
##               Estimate Std. Error         df  t value Pr(&gt;|t|)    
## (Intercept)  17.879803   0.093196  98.998382    191.9   &lt;2e-16 ***
## x           -99.980041   0.009342  99.528150 -10702.0   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##   (Intr)
## x 0.106
</code></pre>

<pre><code class="language-r">anova(model3)
</code></pre>

<pre><code>## Type III Analysis of Variance Table with Satterthwaite's method
##    Sum Sq Mean Sq NumDF  DenDF   F value    Pr(&gt;F)    
## x 1114376 1114376     1 99.528 114532908 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>Again, $\hat{\beta_{0.}} =$ 18.0612368 and $\hat{\beta_{1.}} =$ -100.0019619 were correctly estimated (i.e. 18 vs -100) for level 1. Level 2 estimates for subject 1 were also correctly estimated: $\hat{\beta_{01}}$ = 18.3278073 vs <em>β</em><sub>11</sub> = 18.3177611 and $\hat{\beta_{11}}$ = -99.9649699 vs <em>β</em><sub>11</sub> = -99.9383084.</p>

<h2 id="model-4-random-intercepts-for-each-object-and-each-subject">Model 4: Random intercepts for each object and each subject</h2>

<p>As explained, above each subject rates each of twenty objects. In addition to the varying performance among the participants, one can assume that there are objects, which can be remembered more easily than others. Therefore, I add a random intercept for each object representing at varying baseline memorability.</p>

<p>Level 1:
<em>Y</em><sub>*i<strong>j*</sub> = <em>β</em><sub>0*i</strong>j*</sub> + <em>β</em><sub>1</sub><em>X</em><sub>*i<strong>j*</sub> + <em>e</em><sub>*i</strong>j*</sub></p>

<p>Level 2:
<em>β</em><sub>0*i*<em>j</em></sub> = <em>γ</em><sub>00</sub> + <em>γ</em><sub>01</sub><em>W</em><sub>1<em>i</em></sub> + <em>u</em><sub>01<em>i</em></sub> + <em>γ</em><sub>02</sub><em>W</em><sub>2<em>j</em></sub> + <em>u</em><sub>02<em>j</em></sub></p>

<ul>
<li><em>Y</em><sub>*i*<em>j</em></sub> refers to memory score for object i and subject j.</li>
<li><em>β</em><sub>0*i*<em>j</em></sub> refers to the intercept of the dependent variable that is different for each subject and object.</li>
<li><em>β</em><sub>1</sub> refers to the linear slope.</li>
<li><em>X</em><sub>*i*<em>j</em></sub> refers to the level 1 predictor.</li>
<li><em>e</em><sub>*i*<em>j</em></sub> refers to the random errors of prediction for the level 1 equation.</li>
<li><em>γ</em><sub>00</sub> refers to the overall intercept. This is the grand mean of the scores on the dependent variable across all the subjects when all the predictors are equal to 0.</li>
<li><em>γ</em><sub>01</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub>1<em>i</em></sub> refers to the level 2 predictor for objects.</li>
<li><em>u</em><sub>01<em>i</em></sub> refers to the random error component for the deviation of the intercept of a object from the overall intercept.</li>
<li><em>γ</em><sub>02</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub>2<em>j</em></sub> refers to the level 2 predictor for subjects.</li>
<li><em>u</em><sub>02<em>j</em></sub> refers to the random error component for the deviation of the intercept of a subject from the overall intercept.</li>
</ul>

<pre><code class="language-r"># Generating data set
# General values and variables
numObj &lt;- 20
numSub &lt;- 100
e      &lt;- rnorm(numObj * numSub, mean = 0, sd = 0.1)
x      &lt;- scale(runif(numObj * numSub, min = -100, max = 100))
y      &lt;- c()
index  &lt;- 1

# Coefficients
gamma00 &lt;- 18
gamma01 &lt;- 0.5
gamma02 &lt;- -10
beta1    &lt;- -100
w1       &lt;- runif(numObj, min = 0, max = 3)
w2       &lt;- runif(numSub, min = -3, max = 3)
u01       &lt;- rnorm(numObj, mean = 0, sd = 0.1)
u02       &lt;- rnorm(numSub, mean = 0, sd = 0.1)

for(j in 1:numSub){
  for(i in 1:numObj){
    y[index] &lt;-gamma00 + gamma01*w1[i] + u01[i] + gamma02*w2[j] + u02[j] + beta1*x[index] + e[index]
    index &lt;- index + 1
  } 
}

dataFrame4 &lt;- data.frame(y = y, x = x, subNo = factor(rep(1:numSub, each = numObj)), objNum = factor(rep(1:numObj, numSub))) 

model4 &lt;- lmer(y ~  x + 
                 (1 | subNo) + 
                 (1 | objNum), data = dataFrame4)
summary(model4)
</code></pre>

<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite's method [
## lmerModLmerTest]
## Formula: y ~ x + (1 | subNo) + (1 | objNum)
##    Data: dataFrame4
## 
## REML criterion at convergence: -2138.5
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.7531 -0.6271  0.0010  0.6649  3.5013 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev.
##  subNo    (Intercept) 3.164e+02 17.78779
##  objNum   (Intercept) 1.008e-01  0.31753
##  Residual             9.602e-03  0.09799
## Number of obs: 2000, groups:  subNo, 100; objNum, 20
## 
## Fixed effects:
##               Estimate Std. Error         df   t value Pr(&gt;|t|)    
## (Intercept)  1.945e+01  1.780e+00  9.932e+01     10.93   &lt;2e-16 ***
## x           -1.000e+02  2.271e-03  1.880e+03 -44038.89   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##   (Intr)
## x 0.000
</code></pre>

<pre><code class="language-r">anova(model4)
</code></pre>

<pre><code>## Type III Analysis of Variance Table with Satterthwaite's method
##     Sum Sq  Mean Sq NumDF DenDF    F value    Pr(&gt;F)    
## x 18621423 18621423     1  1880 1939423508 &lt; 2.2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<p>Firstly, on level 1 estimates were close to the true values: $\hat{\beta_{0..}} =$ 19.450424 (vs 19.4497162) as well as $\hat{\beta_{1}} =$ -100.0054394 (vs -100). Now looking at the level 2 estimates: the intercept for the object 1 across all subjects is $\hat{\beta_{01.}}$ = 19.0068543 (vs 18.9956614), while the intercept for subject 1 across all objects is $\hat{\beta_{0.1}}$ = 41.9438705 (vs 41.9422321).</p>

<h2 id="model-5-random-intercepts-for-each-object-each-subject-quadratic-term-and-random-slopes-for-subjects">Model 5: Random intercepts for each object, each subject, quadratic term and random slopes for subjects</h2>

<h3 id="model-5-1">Model 5.1</h3>

<p>I am finally arriving at the model, which I want to use to model my data in the upcoming analysis. This models contains random intercepts for each object and each subject. A second quadratic term is added to test for non-linearities. Both the slope of the linear term and the slope of the quadratic term are random for each subject. At this point it is crucial to note that the predictor <em>X</em> is random for each subject and object just as if the subjects had to rate the expectancy of each object after their memory was tested. This will be the case of my experiment, but I also collected normative data, where <em>X</em> varies only across objects representing aggregate values (see model 5.2).</p>

<p>Level 1:
<em>Y</em><sub>*i<strong>j*</sub> = <em>β</em><sub>0*i</strong>j*</sub> + <em>β</em><sub>1<em>j</em></sub><em>X</em><sub>*i<strong>j*</sub> + <em>β</em><sub>2<em>j</em></sub><em>X</em><sub>*i</strong>j*</sub><sup>2</sup> + <em>e</em><sub>*i<strong>j*</sub>
 Level 2:
<em>β</em><sub>0*i</strong>j*</sub> = <em>γ</em><sub>00</sub> + <em>γ</em><sub>01</sub><em>W</em><sub>1<em>i</em></sub> + <em>u</em><sub>01<em>i</em></sub> + <em>γ</em><sub>02</sub><em>W</em><sub>2<em>j</em></sub> + <em>u</em><sub>02<em>j</em></sub>
<em>β</em><sub>1<em>j</em></sub> = <em>γ</em><sub>10</sub> + <em>u</em><sub>1<em>j</em></sub>
<em>β</em><sub>2<em>j</em></sub> = <em>γ</em><sub>20</sub> + <em>u</em><sub>2<em>j</em></sub></p>

<ul>
<li><em>Y</em><sub>*i*<em>j</em></sub> refers to memory score for object i and subject j.</li>
<li><em>β</em><sub>0*i*<em>j</em></sub> refers to the intercept of the dependent variable that is different for each subject and object.</li>
<li><em>β</em><sub>1<em>j</em></sub> refers to the linear slope for the relationship in subject j (level 2) between the level 1 predictor and the dependent variable.</li>
<li><em>X</em><sub>*i*<em>j</em></sub> refers to the linear level 1 predictor that varies for across objects and subjects.</li>
<li><em>β</em><sub>2<em>j</em></sub> refers to the quadratic slope for the relationship in subject j (level 2) between the level 1 predictor and the dependent variable.</li>
<li><em>X</em><sub>*i*<em>j</em></sub><sup>2</sup> refers to the quadratic level 1 predictor that varies for across objects and subjects.</li>
<li><em>e</em><sub>*i*<em>j</em></sub> refers to the random errors of prediction for the level 1 equation.</li>
<li><em>γ</em><sub>00</sub> refers to the overall intercept. This is the grand mean of the scores on the dependent variable across all the subjects when all the predictors are equal to 0.</li>
<li><em>γ</em><sub>01</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub>1<em>i</em></sub> refers to the level 2 predictor for objects.</li>
<li><em>u</em><sub>01<em>i</em></sub> refers to the random error component for the deviation of the intercept of a object from the overall intercept.</li>
<li><em>γ</em><sub>02</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub>2<em>j</em></sub> refers to the level 2 predictor for subjects.</li>
<li><em>u</em><sub>02<em>j</em></sub> refers to the random error component for the deviation of the intercept of a subject from the overall intercept.</li>
<li><em>γ</em><sub>10</sub> overall linear slope of the level 2.</li>
<li><em>u</em><sub>1<em>j</em></sub> refers to the random error component for the deviation from the linear slope of a subject from the overall intercept.</li>
<li><em>γ</em><sub>20</sub> overall quadratic slope of the level 2.</li>
<li><em>u</em><sub>2<em>j</em></sub> refers to the random error component for the deviation from the quadratic slope of a subject from the overall intercept.</li>
</ul>

<pre><code class="language-r"># Generating data set
# General values
numObj &lt;- 20
numSub &lt;- 100
index  &lt;- 1
y      &lt;- c()
x      &lt;- scale(runif(numObj * numSub, min = -100, max = 100)) # randomly varies across object and subjects as post ratings, but the problem here could be that this assumes that the ratings for the objects across subjects are completely uncorrelated
x2     &lt;- x*x
e      &lt;- rnorm(numObj * numSub, mean = 0, sd = 0.1) 

# Coefficients
gamma00 &lt;- 0.5
gamma01 &lt;- 2
gamma02 &lt;- 8
gamma10 &lt;- 0
gamma20 &lt;- 10

# Varying stuff
w1 &lt;- runif(numObj, min = -4, max = 4)
w2 &lt;- runif(numSub, min =  0, max = 4)
u01 &lt;- rnorm(numObj, mean = 0, sd = 0.1)
u02 &lt;- rnorm(numSub, mean = 0, sd = 0.1)
u1 &lt;- rnorm(numSub, mean = 0, sd = 0.1)
u2 &lt;- rnorm(numSub, mean = 0, sd = 0.1)

for(j in 1:numSub){
  for(i in 1:numObj){
    # Long format
    y[index] &lt;- gamma00 + gamma01 * w1[i]  + u01[i] + gamma02 * w2[j] + u02[j] + (gamma10 + u1[j]) * x[index] + (gamma20 + u2[j]) * x2[index] + e[index]
    index &lt;- index + 1
  }
}

dataFrame5 &lt;- data.frame(y = y, subNo = factor(rep(1:numSub, each = numObj)), objNum = factor(rep(1:numObj, numSub)), x = x, x2 = x2)

model5 &lt;- lmer(y ~  x + 
                    x2 + 
                    (1 | subNo) + 
                    (1 | objNum) + 
                    (x2|subNo) +
                    (x|subNo), data = dataFrame5)

summary(model5)
</code></pre>

<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite's method [
## lmerModLmerTest]
## Formula: y ~ x + x2 + (1 | subNo) + (1 | objNum) + (x2 | subNo) + (x |  
##     subNo)
##    Data: dataFrame5
## 
## REML criterion at convergence: -1542.7
## 
## Scaled residuals: 
##      Min       1Q   Median       3Q      Max 
## -2.90993 -0.61316  0.02424  0.64441  2.92104 
## 
## Random effects:
##  Groups   Name        Variance  Std.Dev. Corr 
##  subNo    (Intercept)  0.296066 0.54412       
##           x            0.013235 0.11504  0.37 
##  subNo.1  (Intercept)  0.627256 0.79199       
##           x2           0.008627 0.09288  -0.46
##  subNo.2  (Intercept) 70.071082 8.37085       
##  objNum   (Intercept) 23.260830 4.82295       
##  Residual              0.009904 0.09952       
## Number of obs: 2000, groups:  subNo, 100; objNum, 20
## 
## Fixed effects:
##              Estimate Std. Error        df  t value Pr(&gt;|t|)    
## (Intercept) 17.495733   1.368575 45.984110   12.784   &lt;2e-16 ***
## x            0.002500   0.011769 98.789562    0.212    0.832    
## x2           9.970179   0.009687 98.504324 1029.206   &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##    (Intr) x     
## x   0.014       
## x2 -0.026  0.001
## convergence code: 0
## unable to evaluate scaled gradient
## Model failed to converge: degenerate  Hessian with 1 negative eigenvalues
</code></pre>

<pre><code class="language-r">anova(model5)
</code></pre>

<pre><code>## Type III Analysis of Variance Table with Satterthwaite's method
##    Sum Sq Mean Sq NumDF  DenDF    F value Pr(&gt;F)    
## x       0       0     1 98.790 4.5100e-02 0.8322    
## x2  10492   10492     1 98.504 1.0593e+06 &lt;2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<pre><code class="language-r">corrResult &lt;- cor.test(coef(model5)$subNo$`(Intercept)`, gamma00 + mean(gamma01*w1 + u01) + gamma02*w2 + u02)
</code></pre>

<p>The intercept of object 1 across all subjects is $\hat{\beta_{01.}} =$ 16.4906087 (vs 16.4837433), while subject&rsquo;s 1 performance across all objects was $\hat{\beta_{0.1}} =$ 17.3720882 (vs 15.6800431). Compared to the other estimates, the last estimate is quite far off. However the estimates are still highly correlated with the true values, <em>r</em> = .185, <em>p</em> = .065. However on some simulations they are not, which I realised after running this script a couple of times. I have no idea why this is the case. On level 2, the linear slope for subject 1 was $\hat{\beta_{11}} =$ -0.0244874 (vs -0.0174013), while the quadratic slope was $\hat{\beta_{21}} =$ 9.9571944 (vs 9.970709).</p>

<h3 id="model-5-2">Model 5.2</h3>

<p>This model is basically the same than the previous except one different. <em>X</em> varies only across objects like as if it was aggregated values from a different population. This is case in the experiment, in which one could use the normative data instead of participants&rsquo; ratings to predict the memory score.</p>

<p>Level 1:
<em>Y</em><sub>*i<strong>j*</sub> = <em>β</em><sub>0*i</strong>j*</sub> + <em>β</em><sub>1<em>j</em></sub><em>X</em><sub><em>i</em></sub> + <em>β</em><sub>2<em>j</em></sub><em>X</em><sub><em>i</em></sub><sup>2</sup> + <em>e</em><sub>*i<strong>j*</sub>
 Level 2:
<em>β</em><sub>0*i</strong>j*</sub> = <em>γ</em><sub>00</sub> + <em>γ</em><sub>01</sub><em>W</em><sub>1<em>i</em></sub> + <em>u</em><sub>01<em>i</em></sub> + <em>γ</em><sub>02</sub><em>W</em><sub>2<em>j</em></sub> + <em>u</em><sub>02<em>j</em></sub>
<em>β</em><sub>1<em>j</em></sub> = <em>γ</em><sub>10</sub> + <em>u</em><sub>1<em>j</em></sub>
<em>β</em><sub>2<em>j</em></sub> = <em>γ</em><sub>20</sub> + <em>u</em><sub>2<em>j</em></sub></p>

<ul>
<li><em>Y</em><sub>*i*<em>j</em></sub> refers to memory score for object i and subject j.</li>
<li><em>β</em><sub>0*i*<em>j</em></sub> refers to the intercept of the dependent variable that is different for each subject and object.</li>
<li><em>β</em><sub>1<em>j</em></sub> refers to the linear slope for the relationship in subject j (level 2) between the level 1 predictor and the dependent variable.</li>
<li><em>X</em><sub><em>i</em></sub> refers to the linear level 1 predictor that varies for across objects but not across subjects.</li>
<li><em>β</em><sub>2<em>j</em></sub> refers to the quadratic slope for the relationship in subject j (level 2) between the level 1 predictor and the dependent variable.</li>
<li><em>X</em><sub><em>i</em></sub><sup>2</sup> refers to the quadratic level 1 predictor that varies for across objects but not across subjects.</li>
<li><em>e</em><sub>*i*<em>j</em></sub> refers to the random errors of prediction for the level 1 equation.</li>
<li><em>γ</em><sub>00</sub> refers to the overall intercept. This is the grand mean of the scores on the dependent variable across all the subjects when all the predictors are equal to 0.</li>
<li><em>γ</em><sub>01</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub>1<em>i</em></sub> refers to the level 2 predictor for objects.</li>
<li><em>u</em><sub>01<em>i</em></sub> refers to the random error component for the deviation of the intercept of a object from the overall intercept.</li>
<li><em>γ</em><sub>02</sub> refers to the overall regression coefficient, or the slope, between the dependent variable and the level 2 predictor.</li>
<li><em>W</em><sub>2<em>j</em></sub> refers to the level 2 predictor for subjects.</li>
<li><em>u</em><sub>02<em>j</em></sub> refers to the random error component for the deviation of the intercept of a subject from the overall intercept.</li>
<li><em>γ</em><sub>10</sub> overall linear slope of the level 2.</li>
<li><em>u</em><sub>1<em>j</em></sub> refers to the random error component for the deviation from the linear slope of a subject from the overall intercept.</li>
<li><em>γ</em><sub>20</sub> overall quadratic slope of the level 2.</li>
<li><em>u</em><sub>2<em>j</em></sub> refers to the random error component for the deviation from the quadratic slope of a subject from the overall intercept.</li>
</ul>

<pre><code class="language-r"># Generating data set
# General values
numObj &lt;- 20
numSub &lt;- 100
index  &lt;- 1
y      &lt;- c()
x      &lt;- scale(runif(numObj, min = -100, max = 100)) # randomly varies across object and subjects as post ratings, but the problem here could be that this assumes that the ratings for the objects across subjects are completely uncorrelated
x2     &lt;- x*x
e      &lt;- rnorm(numObj * numSub, mean = 0, sd = 0.1) 

# Coefficients
gamma00 &lt;- 0.5
gamma01 &lt;- 2
gamma02 &lt;- 8
gamma10 &lt;- 0
gamma20 &lt;- 10

# Varying stuff
w1 &lt;- runif(numObj, min = -4, max = 4)
w2 &lt;- runif(numSub, min =  0, max = 4)
u01 &lt;- rnorm(numObj, mean = 0, sd = 0.1)
u02 &lt;- rnorm(numSub, mean = 0, sd = 0.1)
u1 &lt;- rnorm(numSub, mean = 0, sd = 0.1)
u2 &lt;- rnorm(numSub, mean = 0, sd = 0.1)

for(j in 1:numSub){
  for(i in 1:numObj){
    # Long format
    y[index] &lt;- gamma00 + gamma01 * w1[i]  + u01[i] + gamma02 * w2[j] + u02[j] + (gamma10 + u1[j]) * x[i] + (gamma20 + u2[j]) * x2[i] + e[index]
    index &lt;- index + 1
  }
}

dataFrame5 &lt;- data.frame(y = y, subNo = factor(rep(1:numSub, each = numObj)), objNum = factor(rep(1:numObj, numSub)), x = x, x2 = x2)

model5 &lt;- lmer(y ~  x + 
                    x2 + 
                    (1 | subNo) + 
                    (1 | objNum) + 
                    (x2|subNo) +
                    (x|subNo), data = dataFrame5)

summary(model5)
</code></pre>

<pre><code>## Linear mixed model fit by REML. t-tests use Satterthwaite's method [
## lmerModLmerTest]
## Formula: y ~ x + x2 + (1 | subNo) + (1 | objNum) + (x2 | subNo) + (x |  
##     subNo)
##    Data: dataFrame5
## 
## REML criterion at convergence: -1497.3
## 
## Scaled residuals: 
##     Min      1Q  Median      3Q     Max 
## -3.2316 -0.6402  0.0021  0.6139  3.4157 
## 
## Random effects:
##  Groups   Name        Variance Std.Dev. Corr 
##  subNo    (Intercept)  0.77346 0.8795        
##           x            0.01168 0.1081   -1.00
##  subNo.1  (Intercept) 62.59113 7.9115        
##           x2           0.01023 0.1011   0.30 
##  subNo.2  (Intercept) 18.34655 4.2833        
##  objNum   (Intercept) 23.05402 4.8015        
##  Residual              0.01039 0.1019        
## Number of obs: 2000, groups:  subNo, 100; objNum, 20
## 
## Fixed effects:
##             Estimate Std. Error      df t value Pr(&gt;|t|)    
## (Intercept)  17.7209     1.9922 26.6294   8.895 1.86e-09 ***
## x            -0.1758     1.1925 17.0058  -0.147    0.885    
## x2           10.2781     1.4883 16.9798   6.906 2.56e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Correlation of Fixed Effects:
##    (Intr) x     
## x  -0.272       
## x2 -0.709  0.383
</code></pre>

<pre><code class="language-r">anova(model5)
</code></pre>

<pre><code>## Type III Analysis of Variance Table with Satterthwaite's method
##     Sum Sq Mean Sq NumDF  DenDF F value   Pr(&gt;F)    
## x  0.00023 0.00023     1 17.006  0.0217   0.8845    
## x2 0.49531 0.49531     1 16.980 47.6909 2.56e-06 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
</code></pre>

<pre><code class="language-r">corrResult &lt;- cor.test(coef(model5)$subNo$`(Intercept)`, gamma00 + mean(gamma01*w1 + u01) + gamma02*w2 + u02)
</code></pre>

<p>The intercept of object 1 across all subjects is $\hat{\beta_{01.}} =$ 16.6529841 (vs 16.6423956), while subject&rsquo;s 1 performance across all objects was $\hat{\beta_{0.1}} =$ 17.139073 (vs 20.5890653). Compared to the other estimates, the last estimate is quite far off. However the estimates are still highly correlated with the true values, <em>r</em> = .094, <em>p</em> = .352. However on some simulations they are not, which I realised after running this script a couple of times. I have no idea why this is the case. On level 2, the linear slope for subject 1 was $\hat{\beta_{11}} =$ 0.0037117 (vs -0.1519748), while the quadratic slope was $\hat{\beta_{21}} =$ 9.9792044 (vs 10.2879503).</p>

<h3 id="inaccurate-estimations-of-the-random-intercepts-for-each-subject">Inaccurate estimations of the random intercepts for each subject</h3>

<p>There seems to be a problem in both simulation above that the estimate of $\hat{\beta_{0.j}}$ was quite inaccurate sometimes, but this could be due the fact that I used 16 subjects in previous analyses. To test this, I run number of simulation with the same parameters.</p>

<pre><code class="language-r">n &lt;- 100
# Fixed values:
  numObj &lt;- 20
  numSub &lt;- 100
  gamma00 &lt;- 0.5
  gamma01 &lt;- 2
  gamma02 &lt;- 8
  gamma10 &lt;- 0
  gamma20 &lt;- 10
  corrResults &lt;- c()
  
# Loop for n simulations
for(run in 1:n){
  # Generating data set
  # General values
  index  &lt;- 1
  y      &lt;- c()
  x      &lt;- scale(runif(numObj, min = -100, max = 100)) # randomly varies across object and subjects as post ratings, but the problem here could be that this assumes that the ratings for the objects across subjects are completely uncorrelated
  x2     &lt;- x*x
  e      &lt;- rnorm(numObj * numSub, mean = 0, sd = 0.1) 
  # Varying stuff
  w1 &lt;- runif(numObj, min = -4, max = 4)
  w2 &lt;- runif(numSub, min =  0, max = 4)
  u01 &lt;- rnorm(numObj, mean = 0, sd = 0.1)
  u02 &lt;- rnorm(numSub, mean = 0, sd = 0.1)
  u1 &lt;- rnorm(numSub, mean = 0, sd = 0.1)
  u2 &lt;- rnorm(numSub, mean = 0, sd = 0.1)
  
  for(j in 1:numSub){
    for(i in 1:numObj){
      # Long format
      y[index] &lt;- gamma00 + gamma01 * w1[i]  + u01[i] + gamma02 * w2[j] + u02[j] + (gamma10 + u1[j]) * x[i] + (gamma20 + u2[j]) * x2[i] + e[index]
      index &lt;- index + 1
    }
  }
  
  dataFrame5 &lt;- data.frame(y = y, subNo = factor(rep(1:numSub, each = numObj)), objNum = factor(rep(1:numObj, numSub)), x = x, x2 = x2)
  
  try(model5 &lt;- lmer(y ~  x + 
                      x2 + 
                      (1 | subNo) + 
                      (1 | objNum) + 
                      (x2|subNo) +
                      (x|subNo), data = dataFrame5))
  
  corrResults[run] &lt;- cor.test(coef(model5)$subNo$`(Intercept)`, gamma00 + mean(gamma01*w1 + u01) + gamma02*w2 + u02)$estimate
}
</code></pre>

<p>Even when increasing the number of subjects to 100, there were models that couldn&rsquo;t be estimated or had every low correlations coefficients. See the histogram below for the distribution.</p>

<pre><code class="language-r">ggplot(data.frame(corrResults), aes(corrResults)) + geom_histogram() + theme(panel.margin = unit(1, &quot;cm&quot;), text = element_text(size = 12), plot.margin = margin(10, 10, 10, 10)) + 
        labs(y = 'Frequency', x = 'Correlation coefficients') + 
        coord_cartesian(xlim = c(0, 1), expand = FALSE)
</code></pre>

<p><img src="2018-03-14-mixed-linear-models_files/figure-markdown_github/unnamed-chunk-10-1.png" alt="" /></p>

<h1 id="conclusion">Conclusion</h1>

<p>All models except the last two can be estimated with a decent accuracy. The problem for the last two models is that on rare occasions the model fails to converge or the correlation coefficients for $\hat{\beta_{0.j}}$ and their true value is very low. At the moment, I don&rsquo;t really know what to do about it.</p>


    

    

    <h4>See also</h4>
    <ul>
        
            <li><a href="/2018/01/analysing-normative-data-set/">Analysing normative data set</a></li>
        
            <li><a href="/2018/02/final-analysis-of-pilot-data/">Final analysis of pilot data</a></li>
        
            <li><a href="/2018/01/checking-whether-my-data-is-saved-correctly/">Checking whether my data is saved correctly</a></li>
        
            <li><a href="/2018/01/second-attempt-to-select-set-of-object-location-pairs/">Second attempt to select set of object/location pairs</a></li>
        
            <li><a href="/2018/01/testing-whether-targets-differ-in-rank-from-foils/">Testing whether targets differ in rank from foils</a></li>
        
    </ul>


</article>



        </div>

        <aside class="col-12 col-lg-3 ml-auto blog-sidebar">
    
        


<section>
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">
        
        <li>
            <a href="/2018/11/selecting-and-checking-living-non-living-words/">Selecting and checking living/non-living words</a>
        </li>
        
        <li>
            <a href="/2018/04/3d-location-recall-task-in-vr/">3D location recall task in VR</a>
        </li>
        
        <li>
            <a href="/2018/03/general-update-and-plan-for-the-future/">General update and plan for the future</a>
        </li>
        
        <li>
            <a href="/2018/03/mixed-linear-models/">Mixed linear models</a>
        </li>
        
        <li>
            <a href="/2018/02/final-design-of-schemavr1/">Final design of schemaVR1</a>
        </li>
        
    </ol>
</section>

    
    
        <section>
    
        
    
        
        <h4>Categories</h4>
        <p>
            
            <a class="badge badge-primary" href="/categories/experiments">experiments</a>
            
            <a class="badge badge-primary" href="/categories/general">general</a>
            
        </p>
        
    
        
        <h4>Tags</h4>
        <p>
            
            <a class="badge badge-primary" href="/tags/about">about</a>
            
            <a class="badge badge-primary" href="/tags/data-check">data-check</a>
            
            <a class="badge badge-primary" href="/tags/familiarity">familiarity</a>
            
            <a class="badge badge-primary" href="/tags/graphics">graphics</a>
            
            <a class="badge badge-primary" href="/tags/hippocampus">hippocampus</a>
            
            <a class="badge badge-primary" href="/tags/ideas-and-plans">ideas-and-plans</a>
            
            <a class="badge badge-primary" href="/tags/lab-meeting">lab-meeting</a>
            
            <a class="badge badge-primary" href="/tags/lme4/lmertest">lme4/lmertest</a>
            
            <a class="badge badge-primary" href="/tags/memory">memory</a>
            
            <a class="badge badge-primary" href="/tags/meta">meta</a>
            
            <a class="badge badge-primary" href="/tags/mixed-linear-models">mixed-linear-models</a>
            
            <a class="badge badge-primary" href="/tags/mpfc">mpfc</a>
            
            <a class="badge badge-primary" href="/tags/noveltyvr">noveltyvr</a>
            
            <a class="badge badge-primary" href="/tags/pilot-study">pilot-study</a>
            
            <a class="badge badge-primary" href="/tags/ratingstudy">ratingstudy</a>
            
            <a class="badge badge-primary" href="/tags/recognition-memory">recognition-memory</a>
            
            <a class="badge badge-primary" href="/tags/recollection">recollection</a>
            
            <a class="badge badge-primary" href="/tags/research-design">research-design</a>
            
            <a class="badge badge-primary" href="/tags/rmarkdown">rmarkdown</a>
            
            <a class="badge badge-primary" href="/tags/schema">schema</a>
            
            <a class="badge badge-primary" href="/tags/schemavr">schemavr</a>
            
            <a class="badge badge-primary" href="/tags/schemavr1">schemavr1</a>
            
            <a class="badge badge-primary" href="/tags/unity3d">unity3d</a>
            
            <a class="badge badge-primary" href="/tags/word-properties">word-properties</a>
            
            <a class="badge badge-primary" href="/tags/words">words</a>
            
        </p>
        
    
</section>
    
</aside>

      </div>
    </div>
    

    
      






<footer class="blog-footer w-100">
    <nav class="navbar navbar-light bg-light">
        <p class="w-100 text-center">Hugo template made with ❤ by <a href="https://github.com/Xzya">Xzya</a>, inspired by <a href="https://github.com/alanorth/hugo-theme-bootstrap4-blog">hugo-theme-bootstrap4-blog</a></p>
        <p class="w-100 text-center"><a href="#">Back to top</a></p>
    </nav>
</footer>

    

    
    
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
  </body>
</html>
