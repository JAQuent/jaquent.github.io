---
title: "Interim pilot data of schemaVR1"
author: "JÃ¶rn Alexander Quent"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 6
    theme: united
bibliography: references2.bib
csl: apa-5th-edition.csl
---

```{r setup, include=FALSE}
knitr::opts_knit$set(eval.after = 'fig.cap')
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.width = 10, 
                      fig.height= 7, 
                      dpi=300, 
                      out.width="1200px", 
                      out.height="700px")
options(scipen = 30)
```

## Introduction
The current data collection is part of a broader attempt to pilot the design and to refine my hypotheses. The experiment itself is referred to as schemaVR1. The analyses that are pre-planned and post-hoc are clearly labelled. 

  This script was created to show the interim results. In order to be transparent to which extent I analysed the data before the conclusion of the data collection, I uploaded this version. This especially important because I did not pre-register the study. The aim of this pilot study is to collect sixteen participants in total, which will hopefully be done by the end of the week.

  Personally, I will use this script to look at and check the results as soon as new data from participants is added to the dataset. This script covers only the presentation of results and the analyses. All pre-processing, concatenation and the actual updating of the dataset is done with this [script](https://github.com/JAQuent/schemaVR1/blob/master/updateDataset.R), which also contains a detailed legend explaining all variable names. All relevant files will be uploaded in due course to my GitHub repository [schema-memory-VR](https://github.com/JAQuent/schema-memory-VR). Currently, I am not testing any of the various statistical assumptions, which will be part of the thorough data analysis as soon as I have finished data collection.  
  
  This version is  commented and already tries to the discuss the findings because there already a lot of significant effects. However this by no means anyhting like a registered report or pre-print.

### Procedure
If any one is actually interested, I copied the procedure section from my the draft of my first year report/proposal: 

> After testing a couple of participants, it became apparent that I need to add a familiarisation phase because the experience of immersive VR was as a new experience overwhelming. Therefore the participants had two minutes to get used to the feeling of being in a virtual world by being in the same environment that was used to leanr how to pick up objects (see below).

> The encoding phase of the experiment will start with 45 sec in long exposure to a virtual room of 5.15 virtual meter (vm) by 4.4 vm (height 2.58 vm, which was designed to be a kitchen . A vm corresponds to one meter in the real word. Twenty objects are scattered around at twenty discrete locations so called spawn points. The same object will appear at the same location for all participants, but we collected expectancy values for all 400 combinations. Participants will freely walk around in the virtual room. They will be told that to memorise all moveable objects and their locations. To ensure that, participants will be instructed to count the objects. Furthermore, they are told that they need to make sure to look at the floor and in the corners of the room or otherwise they will miss objects. After the time elapsed, the screen turns black. 

>  The next task serves to eliminate active rehearsal serving the same purpose than the reaction time task in @Lew2016, but also as a practice to pick up object in virtual reality. For that, the participants will stand on a virtual square plane surrounded by an infinite horizon. There are four big cubes (edge length 1 vm) in red, blue, green and yellow standing on the floor and four small cubes (edge length 0.5 vm) in the same colours. The experimenter will give the HTC Vive wand controller to the participants. These will be instructed to pick up the floating small cubes by inserting the controller into the object and continously pressing the trigger button, which attaches the object to the controller. The task is to place the small cubes on the big cubes of the same colour. To do this, the participants need to hold the object above big cube and release the trigger button so that the cube can fall down in the right position. 
  
>  When this is finished, the kitchen will be loaded again and the 3D location recall task will begin. A trial (total number twenty) begins by the instantiation of an object at fixed location in the room, which is the same for all twenty objects. Like the small cubes, the objects are floating above the floor. This was done so that the participants do not have to bend to pick up an object. The participants will be instructed to pick the object up and place it at the location where the object was initially during encoding. The participant will be instructed to tell the experimenter when the final placement was made so that this person can log the response. Only the participant cannot remember the object itself, they are to indicated that they are guessing and the trial is marked as a no memory trial. The participant will be aware that a trial can be restarted with the object is retrievably lost for instance because it is stuck in another object in the VE, which is possible. The order of objects is pseudo-random. The following measures will be saved: placement coordinates (x, y, z), time from start of trial to the time the object is picked up, the time to place the object once it is picked up, the number of attempts to place the object and whether the response was guess (no memory trial) or a true answer. Once each of the twenty objects have been placed, the participants will be instructed to take off the VR goggles and sit on the chair in front of the computer in the same room to start the 3AFC location task. 
  
>  On each trial in this task, the participant will be presented with three pictures showing the same object in three different locations of which the participant has to choose the correct one by pressing respective number on the keyboard, which will be displayed under each picture. After this, the participant will be asked to indicate their confidence for this decision on a 3-point scale (1 = Did not see object, 2 = Guess the object was there, 3 = Know the object was there) by pressing the corresponding key.  The first option should only be used when the participant does not remember seeing this object.
  
> As soon as this task has been completed, participants' task will be asked to rate how likely it is that they would see the specific objects at specific locations in the kitchen they saw earlier. A generally unexpected object may be more or less expected depending on the location. After rating the twenty objects at the three locations, the partiicpant will be asked to rate the general expectancy of those objects anywhere in a kitchen. The object/location ratings by loading screen. The scale ranges from unexpected (-100) to expected (100). Move the mouse to move the slider across the scale and press the left mouse button to make your response. Press the spacebar to start.

## Preliminary stuff
### Libraries
```{r}
library(plyr)
library(ggplot2)
library(lmerTest)
library(gridExtra)
library(grid)
library(knitr)
```

### Data
```{r}
# Loading data
load("data/mergedData/exp1Data.RData")

# Adding the normative object/location rating for each target and foil 1 & 2
combData$objLocTargetNorm <- 99
combData$objLocFoil1Norm  <- 99
combData$objLocFoil2Norm  <- 99
for(i in 1:dim(combData)[1]){
  combData$objLocTargetNorm[i] <- combData[i, paste("loc", combData$targetLocation[i], sep = "")]
  combData$objLocFoil1Norm[i]  <- combData[i, paste("loc", combData$foil1Location[i], sep = "")]
  combData$objLocFoil2Norm[i]  <- combData[i, paste("loc", combData$foil2Location[i], sep = "")]
}
```

### Functions
```{r}
pValue <-function(x, sign = '='){
  # To report a p-value in text.
  if (inherits(x, "lm")){
    s <- summary.lm(x)
    x <- pf(s$fstatistic[1L], s$fstatistic[2L], s$fstatistic[3L], lower.tail = FALSE)
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  } else {
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  }
  return(x.converted)
}

rValue <-function(x){
  # To report a correlation coeffiecient in text
  if (inherits(x, "lm")){
    r.squared <- summary(x)$r.squared
    x.converted <- paste('=',substr(as.character(round(r.squared, 3)), 2,5)) 
  } else {
    if (x < 0){
      x.converted <- paste('= -',substr(as.character(abs(round(x, 3))), 2,5), sep = '') 
    } else {
      x.converted <- paste('=',substr(as.character(abs(round(x, 3))), 2,5)) 
    }
  }
  return(x.converted) 
}

sigStars <- function(x){
  # Adding stars to indicate significance
  stars <- rep("", length(x))
  stars[x < 0.1   & x > 0.05]   <- '.' # trend
  stars[x < 0.05  & x > 0.01]   <- '*'
  stars[x < 0.01  & x > 0.001]  <- '**'
  stars[x < 0.001 & x > 0.0001] <- '***'
  return(stars)
}

createResultTable <- function(x){
  # Creating a nice looking table
  if(inherits(x, "glmerMod")){
    # For glmer table
    xTable        <- summary(x)$coefficients
    xTable        <- data.frame(xTable)
    xTable[, 1]   <- round(xTable[, 1], 2)
    xTable[, 2]   <- round(xTable[, 2], 2)
    xTable[, 3]   <- round(xTable[, 3], 2)
    xTable[, 4]   <- round(xTable[, 4], 4)
    xTable        <- cbind(xTable, sigStars(xTable[, 4]))
    names(xTable) <- c('Estimate', 'SE', 'Z', 'P', 'Sig')
  } else if(inherits(x, 'merModLmerTest')){
    xTable        <- summary(x)$coefficients
    xTable        <- data.frame(xTable)
    xTable[, 1]   <- round(xTable[, 1], 2)
    xTable[, 2]   <- round(xTable[, 2], 2)
    xTable[, 3]   <- round(xTable[, 3], 2)
    xTable[, 4]   <- round(xTable[, 4], 2)
    xTable[, 5]   <- round(xTable[, 5], 4)
    xTable        <- cbind(xTable, sigStars(xTable[, 5]))
    names(xTable) <- c('Estimate', 'SE', 'DF', 'T', 'P', 'Sig')
  } else if(inherits(x, 'anova')){
    if(attributes(x)$heading == "Analysis of Variance Table of type III  with  Satterthwaite \napproximation for degrees of freedom"){
      # Only ANOVA on lmerTest models with Satterthwaite approximation
      xTable        <- data.frame(x)
      xTable[, 1]   <- round(xTable[, 1], 2)
      xTable[, 2]   <- round(xTable[, 2], 2)
      xTable[, 3]   <- round(xTable[, 3], 2)
      xTable[, 4]   <- round(xTable[, 4], 2)
      xTable[, 5]   <- round(xTable[, 5], 2)
      xTable[, 6]   <- round(xTable[, 6], 4)
      xTable        <- cbind(xTable, sigStars(xTable[, 6]))
      names(xTable) <- c('SS', 'MSS', 'nDF', 'dDF', 'F', 'P', 'Sig') 
    } else {
      xTable <- data.frame('######', 'No known model', '######')
      names(xTable) <- c('%%%', '***', '&&&')
    }
  } else {
    xTable <- data.frame('######', 'No known model', '######')
    names(xTable) <- c('%%%', '***', '&&&')
  }
  return(xTable)
}
```

## Population & demographics
```{r}
# Aggregating to get summary of demographics
aggDemo   <- ddply(combData, c('subNum', 'gender'), summarise, age = mean(age))
aggGender <- table(aggDemo$gender)
```
Currently, there are `r dim(aggDemo)[1]` participants in this dataset, of which `r aggGender[1]` are female and `r aggGender[2]` are male. The mean age of the participants is `r round(mean(aggDemo$age), 2)`  (SD = `r round(sd(aggDemo$age), 2)`) years.
Due a handling error, one recall trial was not recorded for one participant. For the same participant, there is also no information available whether the participant actually saw the object (i.e. no memory trial). Nevertheless, I included this person in this analysis. 

## Overall performance
### Analysis of no memory trials and confidence decisions
```{r}
# Legend: 1 = Did not see object| 2 = Guess the object was there | 3 = Know the object was there
table1 <- table(combData$resCon)

# Looking at confidence response for each participant
#table2 <- ddply(combData, c('subNum'), summarise, noMemory = sum(recallNoMemory, na.rm = TRUE), notSeen = table(resCon)[1], guessed = table(resCon)[2], knew = table(resCon)[3])
#kable(table2)
# This information is not available online protect to protect anonymity.

# Looking at no memory trials and confidence responses for each object
table3 <- ddply(combData, c('objName', 'objNum'), summarise, 
                noMemory = sum(recallNoMemory, na.rm = TRUE), 
                notSeen  = table(resCon)[1], 
                guessed  = table(resCon)[2], 
                knew     = table(resCon)[3])

result0 <- cor.test(table3$noMemory, table3$notSeen, method = 'pearson')
kable(table3)
```

The number of trials during recall on which participants indicated that they did not see the object was `r sum(combData$recallNoMemory, na.rm = TRUE)` (`r round(mean(combData$recallNoMemory, na.rm = TRUE)*100)` %). Note that the instructions for one participant were different and therefore the number is not completely accurate.  Furthermore, there were `r table1[1]` 'Did not see object', `r table1[2]` 'Guess the object was there' and `r table1[2]` 'Know the object was there' responses in total. Note that one participant was instructed to report a no memory trial if this person did not see the object or absolutely did not know where to place it. All other participants were instructed to report a no memory trial only if there is no memory of the object itself. Additionally, two participants did not receive the general instruction to begin the encoding phase by broadly looking around in the room to make sure they do not run out of time because they focussed on individual objects. This could explain why their number of no memory trials was quite high. Other than that there seem to be no irregularities except that no memory trials do not occur as often as 3AFC trials, on which the participant indicated that they 'Did not see the object'. The corresponding pearson correlation coefficients is *r* `r rValue( result0$estimate)`, *p* `r pValue(result0$p.value)`. The reason for discordance is not immediately apparent. 

### Memory performance
```{r}
# Creating subsets for analysing recall and 3AFC, for which participants have item memory and then
# aggregating across participants to analyse overall memory performance
recallCombDataSub <- subset(combData, combData$recallNoMemory == 0 | is.na(combData$recallNoMemory))
agg1              <- ddply(recallCombDataSub, 
                           c('subNum'), 
                           summarise, 
                           accRecall = mean(accRecall, na.rm = TRUE))

afcCombDataSub    <- subset(combData, combData$resCon != 1)
agg2              <- ddply(afcCombDataSub, 
                           c('subNum'), 
                           summarise, 
                           accAFC = mean(accAFC))


plot1 <- ggplot(agg1, aes(x = "accRecall" , y = accRecall)) + 
                geom_boxplot(alpha = 0.5, width = 0.5) +
                geom_dotplot(binaxis='y', stackdir='center') +
                labs(y = 'Recall rate', 
                     x = ' ', 
                     title = 'A) Recall performance') + 
                coord_cartesian(ylim = c(0, 1), xlim = c(0.9, 1.1)) + 
                theme(plot.margin = margin(10, 10, 10, 10), 
                      axis.title.x=element_blank(),
                      axis.text.x=element_blank(),
                      axis.ticks.x=element_blank())

plot2 <- ggplot(agg2, aes(x = "accAFC" , y = accAFC)) + 
                geom_boxplot(alpha = 0.5, width = 0.5) +
                geom_dotplot(binaxis='y', stackdir='center') + 
                geom_hline(yintercept = 1/3) + 
                annotate('text', x = 0.95, y = (1/3) - 0.03, label = 'Chance (1/3)') + 
                labs(y = 'Accuracy (3AFC)', 
                     x = ' ', 
                     title = 'B) 3AFC performance') + 
                coord_cartesian(ylim = c(0, 1), xlim = c(0.9, 1.1)) + 
                theme(plot.margin = margin(10, 10, 10, 10), 
                      axis.title.x=element_blank(),
                      axis.text.x=element_blank(),
                      axis.ticks.x=element_blank())

# Arrange in grid and plot
grid.arrange(plot1, plot2, ncol = 2)
```

#### Recall accuracy
```{r}
result1 <- t.test(agg1$accRecall)
```
Accuracy of recall was calculated by finding the shortest Euclidean distance and checking whether the location that has the shortest distance is the target location. The mean recall rate was `r round(mean(agg1$accRecall), 2)` (SD = `r round(sd(agg1$accRecall), 2)`), which is significantly above zero, *t*(`r result1$parameter`) = `r round(result1$statistic, 2)`, *p* `r pValue(result1$p.value)`. In contrast to the 3AFC task, it is not straightforward to calculate a chance niveau for the recall task. Therefore I used zero. 

#### Recall bias
```{r}
# Finding the expectancy of the closest location for an object
combData$closestObjLocNorm <- NA
# Add normative location rating
for(i in 1:dim(combData)[1]){
  try(combData$closestObjLocNorm[i] <- combData[i, paste("loc", combData$closestLoc[i], sep = "")])
  # It was necessary to use try() because there are NA values, which cannot be used to index. 
}

recallBiasData <- ddply(subset(combData, recallNoMemory == 0), 
                        c('subNum', 'accRecall'), 
                        summarise, 
                        closestObjLocNorm = mean(closestObjLocNorm, na.rm = TRUE))

recallBias     <- ddply(recallBiasData, 
                        c('accRecall'), 
                        summarise, 
                        mean = mean(closestObjLocNorm, na.rm = TRUE), 
                        sd = sd(closestObjLocNorm, na.rm = TRUE))
```
One idea was that if participants did not remember the location for particular object, they would place it at an expected location because they are biased to these location during retrieval. However the current the mean normative object/location expectancy is lower for incorrectly placed objects, `r round(recallBias[1, 2], 2)` (SD = `r round(recallBias[1, 3], 2)`), than for correctly placed objects, `r round(recallBias[2, 2], 2)` (SD = `r round(recallBias[2, 3], 2)`). Therefore there is no evidence for a recall bias to more expected location. 

#### 3AFC accuracy
```{r}
result2 <- t.test(agg2$accAFC - 1/3)
```
The mean 3AFC accuracy rate was `r round(mean(agg2$accAFC), 2)` (SD = `r round(sd(agg2$accAFC), 2)`), which is significantly higher than `r round(1/3, 2)`, *t*(`r result2$parameter`) = `r round(result2$statistic, 2)`, *p* `r pValue(result2$p.value)`. 

### Relationship between recall and 3AFC performance
```{r}

plot3 <- ggplot(data.frame(accRecall = agg1$accRecall, accAFC = agg2$accAFC), aes(x  = accRecall, y = accAFC)) +
  geom_smooth(method = 'lm') + 
  geom_point() + 
  labs(y = '3AFC accuracy', 
       x = 'Recall rate', 
       title = 'Relationship between memory measures')

result3 <- cor.test(agg1$accRecall, agg2$accAFC)
plot3
```
Performance in the recall task is significantly correlated with performance in the 3AFC task, *r* `r rValue( result3$estimate)`, *p* `r pValue(result3$p.value)`. This important because I was not completely sure whether the way I calculated recall accuracy is correct and sensitive enough to measure what I want to measure because the possible locationsare not, for instance, equidistant and moreover there are a some locations closely cluster in areas of the virtual kitchen. 

## Analysing expectancy
```{r}
# Aggregating over all objects without bothering whether the participants actually saw the objects. This however is done for the statistical analysis (see below). 
objectAgg <- ddply(combData, 
                   c('objNum', 'objName'), 
                   summarise, 
                   generalRatingNorm  = mean(generalRatingNorm),
                   generalRatingPost  = mean(generalRatingPost),
                   afc                = mean(accAFC),
                   targetRankPre      = mean(targetRankPre),
                   objLocTargetRating = mean(objLocTargetRating),
                   recall             = mean(accRecall, na.rm = TRUE), 
                   objLocTargetNorm   = mean(objLocTargetNorm),
                   euclideanDist      = mean(accRecall, na.rm = TRUE),
                   answerTime         = mean(answerTime, na.rm = TRUE))

# Displaying relationship between average object/location expectancy values from nomative and participant data.
plot4 <- ggplot(objectAgg, aes(x  = objLocTargetNorm, y = objLocTargetRating)) +
                geom_smooth(method = 'lm') + 
                geom_point() + 
                labs(y = 'Participant data', 
                     x = 'Normative data', 
                     title = 'A) Object/location expectancy')

# Displaying relationship between average general expectancy values from nomative and participant data.
plot5 <- ggplot(objectAgg, aes(x  = generalRatingNorm, y = generalRatingPost)) +
                geom_smooth(method = 'lm') + 
                geom_point() + 
                labs(y = 'Participant data', 
                     x = 'Normative data', 
                     title = 'B) General expectancy')

# Calculating the correlations  
result4 <- cor.test(objectAgg$objLocTargetNorm, objectAgg$objLocTargetRating)
result5 <- cor.test(objectAgg$generalRatingNorm, objectAgg$generalRatingPost)

# Plot
grid.arrange(plot4, plot5, ncol = 2)
```

For object/location expectancy, the correlation between normative data and average participants ratings for objects at their encoding locations is not significant, *r* `r rValue( result4$estimate)`, *p* `r pValue(result4$p.value)`, which is not completely surprising because the participants in the normative study rated a much higher number of stimuli and did not complete a memory test. On the other, the correlation for the general expectancy values for objects at their encoding locations is significant and very high, *r* `r rValue(result5$estimate)`, *p* `r pValue(result5$p.value)`, which might be explained due to the fact that both groups of participants rated same number of objects in terms of their general expectancy. 

## Analysing effect of expectancy on memory
Note that the subsequent plots are only illustrations of the models because they averaged data across objects, while all the statistical models are trial based and only trials go into the analyses, on which the participant did not indicated that they did not see the object in other words have item memory. 

  As for my specific predictions, this is a quote from a draft of my first year report, where I specify my hypotheses:
  
> My main hypothesis is that there is an U-shape relationship between schema-expectancy and memory. However when recall is used as an assay to test memory, there is always the confound that participants might be biased to the more expected locations. In that respect, I predict to see that memory precision is enhanced for highly expected object/locations. However if the participants does not remember the location the participant is more likely to place the object close to an expected location (i.e. a spawn point) than to an unexpected location. In contrast, I predict a U-shaped relationship for performance in the 3AFC task (unexpected > neutral < expected) because the retrieval effects are reduced as all three options are chosen to have similar expectancy values. 
  
  Concerning the models used in the experiments: my plan/idea was originally to include random intercepts and slopes for the objects and for the participants, but in simulations I had problems to actually retrieve the true parameters ([see here](https://jaquent.github.io/2018/0314_mixedModels.html)). Furthermore, my supervisor pointed out that adding random effects for the objects might take away the effect of their expectancy. Therefore, I, for now, decided to only include a random intercept for each participant. The quadratic term is included to look for the predicted U-shaped relationship. However of course, a significant quadratic term amount to a U-shaped relationship. 
  
  As you will see, sometimes I get the exact opposite result of what I have expected (see e.g. model 10). One of the confound for these models is that all the objects which are expected in a kitchen are at the end of the scale in terms of their object/location expectancy, but all, by definition, highly expected in a kitchen in general, which seems to be associated with lower memory performance. Therefore, I *post-hoc* decided to add the corresponding other forms of expectancy to the model as a covariate. There will also be a second experiment, where objects that are highly expected in kitchen over the whole range of object/location expectancy. All the other models expect the ones predicting the time to place the objects were pre-planned. I looked at this variable because in a recent study using immersive VR from @Draschkow2017 an interesting relationship between 'scene grammar' and object handling time was found. 

```{r}
# Preparation
# scaling for analysis
combDataScaled                    <- combData
combDataScaled$objLocTargetRating <- scale(combData$objLocTargetRating)
combDataScaled$targetRankPre      <- scale(combData$targetRankPre)
combDataScaled$generalRatingNorm  <- scale(combData$generalRatingNorm)
combDataScaled$generalRatingPost  <- scale(combData$generalRatingPost)
combDataScaled$objLocTargetNorm   <- scale(combDataScaled$objLocTargetNorm)

# Adding kitchen relevance. Note that 12 objects were choosen because they are expected in a kitchen, while the other 8 object were not expected in a kitchen.
objectAgg$expectedInKitchen                                  <- 'low'
objectAgg[which(objectAgg$objNum < 13), 'expectedInKitchen'] <- 'highly'
```

### 3AFC models
```{r}
# Plots
# Participant object/location expectancy versus 3AFC
plot6 <- ggplot(objectAgg, aes(x = objLocTargetRating, y = afc)) + 
                geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                geom_smooth() +  
                labs(y = 'Mean accuracy (3AFC)', 
                     x = "Post-ratings expectancy", 
                     title = 'Model 1: Object/location expectancy') +
                coord_cartesian(ylim = c(0, 1.25), xlim = c(-100, 100), expand = TRUE) + 
                theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = c(1.25, 0.8))

plot7 <- ggplot(objectAgg, aes(x = targetRankPre, y = afc)) + 
                geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                geom_smooth() +  
                labs(y = 'Mean accuracy (3AFC)', 
                     x = "Normative expectancy (ranked)", 
                     title = 'Model 2: Object/location expectancy') + 
                coord_cartesian(ylim = c(0, 1.25), xlim = c(0, 400), expand = TRUE) + 
                theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot8 <- ggplot(objectAgg, aes(x = objLocTargetNorm, y = afc)) + 
                geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                geom_smooth() +  
                labs(y = 'Mean accuracy (3AFC)', 
                     x = "Normative expectancy ", 
                     title = 'Model 3: Object/location expectancy') + 
                coord_cartesian(ylim = c(0, 1.25), xlim = c(-100, 100), expand = TRUE) + 
                theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot9 <- ggplot(objectAgg, aes(x = generalRatingNorm, y = afc)) + 
                geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                geom_smooth() +  
                labs(y = 'Mean accuracy (3AFC)', 
                     x = "Normative expectancy", 
                     title = 'Model 4: General expectancy') + 
                coord_cartesian(ylim = c(0, 1.25), xlim = c(-100, 100), expand = TRUE) + 
                theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")


grid.arrange(plot6, plot7, plot8, plot9, ncol = 2, nrow = 2)

# Legend
paste(as.character(objectAgg$objNum), "=", objectAgg$objName)
```


```{r}
# Data
subAFC    <- subset(combDataScaled, combDataScaled$resCon != 1)

# Models 
# Participant object/location expectancy versus 3AFC
model1 <- glmer(accAFC ~  objLocTargetRating + I(objLocTargetRating*objLocTargetRating) + (1 | subNum), 
                    data = subAFC, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)
kable(createResultTable(model1))

# Normative (ranked) object/location expectancy versus 3AFC
model2 <- glmer(accAFC ~  targetRankPre + I(targetRankPre*targetRankPre) + (1 | subNum), 
                    data = subAFC, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model2))

# Normative object/location expectancy versus 3AFC
model3 <- glmer(accAFC ~  objLocTargetNorm + I(objLocTargetNorm*objLocTargetNorm) + (1 | subNum), 
                    data = subAFC, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model3))

# Normative general expectancy versus 3AFC
model4 <- glmer(accAFC ~  generalRatingNorm + I(generalRatingNorm*generalRatingNorm)  + (1 | subNum), 
                    data = subAFC, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model4))
```

#### 3AFC models with covariates
This is was decided post-hoc. They idea here was to adjust for the respective general or object/location expectancy. Another idea would be to control for answer time. 
```{r}
model5 <- glmer(accAFC ~  objLocTargetRating + 
                     I(objLocTargetRating*objLocTargetRating) + 
                     generalRatingPost +
                     (1 | subNum), data = combDataScaled, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model5))

model6 <- glmer(accAFC ~  targetRankPre + 
                     I(targetRankPre*targetRankPre) + 
                     generalRatingNorm +
                     (1 | subNum), data = combDataScaled, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model6))

model7 <- glmer(accAFC ~  objLocTargetNorm + 
                     I(objLocTargetNorm*objLocTargetNorm) + 
                     generalRatingNorm +
                     (1 | subNum), data = combDataScaled, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model7))


model8 <- glmer(accAFC ~  generalRatingNorm + 
                     I(generalRatingNorm*generalRatingNorm) + 
                     objLocTargetNorm +
                     (1 | subNum), data = combDataScaled, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model8))
```

#### 3AFC interpretation
The most robust result is that 3AFC performance is much higher for objects that are generally unexpected in a kitchen. All forms of general expectancy are negatively associated with 3AFC perfomance. Notably though in model 3, there is a trend for a U-shaped relationship. 

### Recall models
The actual statistics are calculated only on trials, which were no no memory trials. 

```{r}
plot10 <- ggplot(objectAgg, aes(x = objLocTargetRating, y = recall)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) +  
                 geom_smooth() +  
                 labs(y = 'Mean accuracy (Recall)', 
                      x = "Post-ratings expectancy", 
                      title = ' Model 9: Object/location expectancy')  + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = c(1.25, 0.8))

plot11 <- ggplot(objectAgg, aes(x = targetRankPre, y = recall)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Mean accuracy (Recall)', 
                      x = "Normative expectancy (ranked)", 
                      title = 'Model 10: Object/location expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(0, 400), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot12 <- ggplot(objectAgg, aes(x = objLocTargetNorm, y = recall)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Mean accuracy (Recall)', 
                      x = "Normative expectancy", 
                      title = 'Model 11: Object/location expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot13 <- ggplot(objectAgg, aes(x = generalRatingNorm, y = recall)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Mean accuracy (Recall)', 
                      x = "Normative expectancy", 
                      title = 'Model 12: General expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")


plot14 <- ggplot(objectAgg, aes(x = generalRatingPost, y = recall)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Mean accuracy (Recall)', 
                      x = "Post-ratings expectancy", 
                      title = 'Model 13: General expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

grid.arrange(plot10, plot11, plot12, plot13, plot14, ncol = 2, nrow = 3)

# Legend
paste(as.character(objectAgg$objNum), "=", objectAgg$objName)
```

```{r}
# Data
subRecall <- subset(combDataScaled, combDataScaled$recallNoMemory == 0)

# Participant object/location expectancy versus recall
model9 <- glmer(accRecall ~ objLocTargetRating + 
                      I(objLocTargetRating*objLocTargetRating) + 
                      (1 | subNum), 
                    data = subRecall, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model9))

# Normative (ranked) object/location expectancy versus recall
model10 <- glmer(accRecall ~  targetRankPre + 
                       I(targetRankPre*targetRankPre) + 
                       (1 | subNum), 
                     data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                     nAGQ = 1)

kable(createResultTable(model10))

# Normative object/location expectancy versus recall
model11 <- glmer(accRecall ~  objLocTargetNorm + 
                       I(objLocTargetNorm*objLocTargetNorm) + 
                       (1 | subNum), 
                     data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                     nAGQ = 1)

kable(createResultTable(model11))

# Normative general expectancy versus recall
model12 <- glmer(accRecall ~  generalRatingNorm + 
                       I(generalRatingNorm*generalRatingNorm)  + 
                       (1 | subNum), data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                     nAGQ = 1)

kable(createResultTable(model12))

# Participant general expectancy versus recall
model13 <- glmer(accRecall ~  generalRatingPost + 
                       I(generalRatingPost*generalRatingPost)  + 
                       (1 | subNum), 
                     data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                   nAGQ = 1)

kable(createResultTable(model13))
```

#### Recall models with covariates
```{r}
# Participant object/location expectancy versus recall
model14 <- glmer(accRecall ~  
                      objLocTargetRating + 
                      I(objLocTargetRating*objLocTargetRating) + 
                      generalRatingPost +
                      (1 | subNum), 
                    data = subRecall, 
                    family = binomial, 
                    control = glmerControl(optimizer = "bobyqa"),
                    nAGQ = 1)

kable(createResultTable(model14))

# Normative (ranked) object/location expectancy versus recall
model15 <- glmer(accRecall ~  targetRankPre + 
                       I(targetRankPre*targetRankPre) + 
                       generalRatingNorm +
                       (1 | subNum), 
                     data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                     nAGQ = 1)

kable(createResultTable(model15))

# Normative object/location expectancy versus recall
model16 <- glmer(accRecall ~  objLocTargetNorm + 
                       I(objLocTargetNorm*objLocTargetNorm) + 
                       generalRatingNorm +
                       (1 | subNum), 
                     data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                     nAGQ = 1)

kable(createResultTable(model16))

# Normative general expectancy versus recall
model17 <- glmer(accRecall ~  generalRatingNorm + 
                       I(generalRatingNorm*generalRatingNorm)  + 
                       objLocTargetNorm +
                       (1 | subNum), 
                     data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                     nAGQ = 1)

kable(createResultTable(model17))

# Participant general expectancy versus recall
model18 <- glmer(accRecall ~  generalRatingPost + 
                       I(generalRatingPost*generalRatingPost)  + 
                       objLocTargetRating +
                       (1 | subNum), 
                     data = subRecall, 
                     family = binomial, 
                     control = glmerControl(optimizer = "bobyqa"),
                     nAGQ = 1)

kable(createResultTable(model18))
```

#### Recall interpretation
Model 9 and 10 show a very unexpected inverted U-shaped relationship. However this might be due to the fact that again objects that are generally unexpected in kitchen are associated with better memory because these are the ones which are in the middle of the object/location expectancy scale. Model 11 looks very similar to model 3. When controlled for general expectancy a lot of models do show an trend for significant quadratic term (model 14 and 15).

### Euclidean distance models
```{r}
plot15 <- ggplot(objectAgg, aes(x = objLocTargetRating, y = euclideanDist)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) +  
                 geom_smooth() +  
                 labs(y = 'Euclidean distance in vm', 
                      x = "Post-ratings expectancy", 
                      title = ' Model 19: Object/location expectancy')  + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = c(1.25, 0.8))

plot16 <- ggplot(objectAgg, aes(x = targetRankPre, y = euclideanDist)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Euclidean distance in vm', 
                      x = "Normative expectancy (ranked)", 
                      title = 'Model 20: Object/location expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(0, 400), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot17 <- ggplot(objectAgg, aes(x = objLocTargetNorm, y = euclideanDist)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Euclidean distance in vm', 
                      x = "Normative expectancy", 
                      title = 'Model 21: Object/location expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot18 <- ggplot(objectAgg, aes(x = generalRatingNorm, y = euclideanDist)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Euclidean distance in vm', 
                      x = "Normative expectancy", 
                      title = 'Model 22: General expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot19 <- ggplot(objectAgg, aes(x = generalRatingPost, y = euclideanDist)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Euclidean distance in vm', 
                      x = "Post-ratings expectancy", 
                      title = 'Model 23: General expectancy') + 
                 coord_cartesian(ylim = c(0, 1), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

grid.arrange(plot15, plot16, plot17, plot18, plot19, ncol = 2, nrow = 3)

# Legend
paste(as.character(objectAgg$objNum), "=", objectAgg$objName)
```


```{r}
# Model 19: 
model19 <- lmer(euclideanDist ~  objLocTargetRating + 
                  I(objLocTargetRating*objLocTargetRating) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model19))

# Model 20: 
model20 <- lmer(euclideanDist ~  targetRankPre + 
                  I(targetRankPre*targetRankPre) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model20))

# Model 21: 
model21 <- lmer(euclideanDist ~  objLocTargetNorm + 
                  I(objLocTargetNorm*objLocTargetNorm) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model21))

# Model 22: 
model22 <- lmer(euclideanDist ~  generalRatingNorm + 
                  I(generalRatingNorm*generalRatingNorm) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model22))

# Model 23: 
model23 <- lmer(euclideanDist ~  generalRatingPost + 
                  I(generalRatingPost*generalRatingPost) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model23))
```

#### Euclidean distance models with covariates
```{r}
# Model 24: 
model24<- lmer(euclideanDist ~  objLocTargetRating + 
                  I(objLocTargetRating*objLocTargetRating) + 
                  generalRatingPost +
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model24))

# Model 25: 
model25 <- lmer(euclideanDist ~  targetRankPre + 
                  I(targetRankPre*targetRankPre) + 
                  generalRatingNorm + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model25))

# Model 26: 
model26 <- lmer(euclideanDist ~  objLocTargetNorm + 
                  I(objLocTargetNorm*objLocTargetNorm) + 
                  generalRatingNorm + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model26))

# Model 27: 
model27 <- lmer(euclideanDist ~  generalRatingNorm + 
                  I(generalRatingNorm*generalRatingNorm) + 
                  objLocTargetNorm +
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model27))

# Model 28: 
model28 <- lmer(euclideanDist ~  generalRatingPost + 
                  I(generalRatingPost*generalRatingPost) + 
                  objLocTargetRating +
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model28))
```

#### Euclidean distance interpretation
The same is true if the Euclidean distance is the dependent variable. Model 24 is interesting as it continues to show the signifcant quadratic effect even though the general expectancy is controlled for. 

### Answer time models
```{r}
plot15 <- ggplot(objectAgg, aes(x = objLocTargetRating, y = answerTime)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) +  
                 geom_smooth() +  
                 labs(y = 'Time to place object in sec', 
                      x = "Post-ratings expectancy", 
                      title = ' Model 19: Object/location expectancy')  + 
                 coord_cartesian(ylim = c(0, 20), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = c(1.25, 0.8))

plot16 <- ggplot(objectAgg, aes(x = targetRankPre, y = answerTime)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Time to place object in sec', 
                      x = "Normative expectancy (ranked)", 
                      title = 'Model 20: Object/location expectancy') + 
                 coord_cartesian(ylim = c(0, 20), xlim = c(0, 400), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot17 <- ggplot(objectAgg, aes(x = objLocTargetNorm, y = answerTime)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Time to place object in sec', 
                      x = "Normative expectancy", 
                      title = 'Model 21: Object/location expectancy') + 
                 coord_cartesian(ylim = c(0, 20), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot18 <- ggplot(objectAgg, aes(x = generalRatingNorm, y = answerTime)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Time to place object in sec', 
                      x = "Normative expectancy", 
                      title = 'Model 22: General expectancy') + 
                 coord_cartesian(ylim = c(0, 20), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

plot19 <- ggplot(objectAgg, aes(x = generalRatingPost, y = answerTime)) + 
                 geom_text(aes(label = objNum, colour = expectedInKitchen), hjust = 0, vjust = 0) + 
                 geom_smooth() +  
                 labs(y = 'Time to place object in sec', 
                      x = "Post-ratings expectancy", 
                      title = 'Model 23: General expectancy') + 
                 coord_cartesian(ylim = c(0, 20), xlim = c(-100, 100), expand = TRUE) + 
                 theme(plot.margin = unit(c(1,7,1,1), "lines"), legend.position = "none")

grid.arrange(plot15, plot16, plot17, plot18, plot19, ncol = 2, nrow = 3)

# Legend
paste(as.character(objectAgg$objNum), "=", objectAgg$objName)
```


```{r}
# Model 19: 
model19 <- lmer(answerTime ~  objLocTargetRating + 
                  I(objLocTargetRating*objLocTargetRating) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model19))

# Model 20: 
model20 <- lmer(answerTime ~  targetRankPre + 
                  I(targetRankPre*targetRankPre) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model20))

# Model 21: 
model21 <- lmer(answerTime ~  objLocTargetNorm + 
                  I(objLocTargetNorm*objLocTargetNorm) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model21))

# Model 22: 
model22 <- lmer(answerTime ~  generalRatingNorm + 
                  I(generalRatingNorm*generalRatingNorm) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model22))
#kable(createResultTable(anova(model22)))

# Model 23: 
model23 <- lmer(answerTime ~  generalRatingPost + 
                  I(generalRatingPost*generalRatingPost) + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model23))
```

#### Answer time models with covariates
```{r}
# Model 24: 
model24<- lmer(answerTime ~  objLocTargetRating + 
                  I(objLocTargetRating*objLocTargetRating) + 
                  generalRatingPost +
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model24))

# Model 25: 
model25 <- lmer(answerTime ~  targetRankPre + 
                  I(targetRankPre*targetRankPre) + 
                  generalRatingNorm + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model25))

# Model 26: 
model26 <- lmer(answerTime ~  objLocTargetNorm + 
                  I(objLocTargetNorm*objLocTargetNorm) + 
                  generalRatingNorm + 
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model26))

# Model 27: 
model27 <- lmer(answerTime ~  generalRatingNorm + 
                  I(generalRatingNorm*generalRatingNorm) + 
                  objLocTargetNorm +
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model27))

# Model 28: 
model28 <- lmer(answerTime ~  generalRatingPost + 
                  I(generalRatingPost*generalRatingPost) + 
                  objLocTargetRating +
                  (1 | subNum), 
                data = subRecall)

kable(createResultTable(model28))
```

#### Answer time interpretation
Here only models that include the respetive covariate are significant but with interesting resuls. For instance model 25 show that when controlled for general expectancy there is an inverted U-shaped relationship, which needs to be inverted if one assummes that shorter answer time are indicative of better memory. 

## Further analysis
### Factorisation
Here I tried the classical way of analysing creating factor (unexpected, neutral and expected) and then aggregating across participants and that factor. 

```{r}
# Based on post-encoding ratings
combData2                      <- combData
combData2$postRatingTargetRank <- ddply(combData2, 
                                        c('subNum'), 
                                        summarise, 
                                        objLocTargetRating = objLocTargetRating, 
                                        rank = rank(objLocTargetRating))$rank

combData2$expFact <- 2
combData2$expFact[which(combData2$postRatingTargetRank <= 7)]  <- 1
combData2$expFact[which(combData2$postRatingTargetRank >= 14)] <- 3
combData2$expFact <- factor(combData2$expFact, labels = c("unexpected", "neutral", "expected"))

combData2Exp <- ddply(combData2, 
                      c('subNum', 'expFact'), 
                      summarise,
                      N = length(accAFC),
                      mean = mean(accAFC),
                      ratingsMean = mean(objLocTargetRating))

# It is important to aggregate data, which was already aggregated.
combData2ExpAgg <- ddply(combData2Exp, 
                         c('expFact'), 
                         summarise,
                         N = length(mean),
                         accAfcMean = mean(mean),
                         accAfcSd = sd(mean),
                         accAfcSe = accAfcSd/sqrt(N))

ggplot(combData2Exp, aes(x = expFact, y = mean)) +
  geom_dotplot(binaxis='y', stackdir='center') +
  stat_summary(fun.y = mean, geom = "line", aes(group=1))  +
  labs(y = 'Mean accuracy (3AFC)', 
       x = "Expectancy", 
       title = 'Relationship with post-encoding ratings') + 
  theme(plot.margin = margin(10, 10, 10, 10))
```

```{r}
# Based on post-encoding ratings
combData3               <- combData
combData3$targetRankPre <- ddply(combData3, 
                                 c('subNum'), 
                                 summarise, 
                                 targetRankPre = targetRankPre, 
                                 rank = rank(targetRankPre))$rank

combData3$expFact <- 2
combData3$expFact[which(combData3$targetRankPre <= 7)] <- 1
combData3$expFact[which(combData3$targetRankPre >= 14)] <- 3
combData3$expFact <- factor(combData3$expFact, labels = c("unexpected", "neutral", "expected"))

combData3Exp <- ddply(combData3, 
                      c('subNum', 'expFact'), 
                      summarise,
                      N = length(accAFC),
                      mean = mean(accAFC),
                      ratingsMean = mean(targetRankPre))

# It is important to aggregate data, which was already aggregated.
combData3ExpAgg <- ddply(combData3Exp, 
                         c('expFact'), 
                         summarise,
                         N = length(mean),
                         accAfcMean = mean(mean),
                         accAfcSd = sd(mean),
                         accAfcSe = accAfcSd/sqrt(N))

ggplot(combData3Exp, aes(x = expFact, y = mean)) +
   geom_dotplot(binaxis='y', stackdir='center') +
  stat_summary(fun.y = mean, geom = "line", aes(group=1))  +
  labs(y = 'Mean accuracy (3AFC)', 
       x = "Expectancy", 
       title = 'Relationship with pre-encoding ranks') + 
  theme(plot.margin = margin(10, 10, 10, 10))
```

## References