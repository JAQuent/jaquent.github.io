---
title: "Additional analysis"
author: "JÃ¶rn Alexander Quent"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 6
    theme: united
---

```{r setup, include=FALSE}
options(scipen = 30)

# Libraries
library(plyr)
library(data.table)
library(knitr)
library(lmerTest)
library(ggplot2)
library(gridExtra)
library(grid)
library(psych)

# Settings
opts_chunk$set(echo = TRUE,
               warning = FALSE,
               message = FALSE,
               eval = TRUE,                     
               fig.width = 10, 
               fig.height = 7, 
               dpi = 300)
opts_knit$set(eval.after = 'fig.cap')

```

# Introduction
I recently submitted my first report, where I reported the results of two experiments (schemaVR1 and schemaVR2) that I ran so far. The analysis of the data strongly suggested that there is an U-shaped relationship between how expected an object is a certain location and memory peformance for that object. In meetings with my supervisor, a couple of things came up that are tested here to make sure that we did not miss a confound. 

```{r, include=FALSE}
pValue <-function(x, sign = '='){
  if (inherits(x, "lm")){
    s <- summary.lm(x)
    x <- pf(s$fstatistic[1L], s$fstatistic[2L], s$fstatistic[3L], lower.tail = FALSE)
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x > 0){
      stop("There is no p-value smaller than 0")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  } else {
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  }
  return(x.converted)
}

rValue <-function(x){
  if (inherits(x, "lm")){
    r.squared <- summary(x)$r.squared
    x.converted <- paste('=',substr(as.character(round(r.squared, 3)), 2,5)) 
  } else {
    if (x < 0){
      x.converted <- paste('= -',substr(as.character(abs(round(x, 3))), 2,5), sep = '') 
    } else {
      x.converted <- paste('=',substr(as.character(abs(round(x, 3))), 2,5)) 
    }
  }
  return(x.converted) 
}

num2word <- function(x){
  # Checking whether number if integer
  if(x%%1 != 0){
    stop('Number is not in integer')
  } else {
    if(x == 0){out <- 'zero'} 
    else if(x == 1){out  <- 'one'} 
    else if(x == 2){out  <- 'two'} 
    else if(x == 3){out  <- 'three'} 
    else if(x == 4){out  <- 'four'} 
    else if(x == 5){out  <- 'five'} 
    else if(x == 6){out  <- 'six'} 
    else if(x == 7){out  <- 'seven'} 
    else if(x == 8){out  <- 'eight'} 
    else if(x == 9){out  <- 'nine'} 
    else if(x == 10){out <- 'ten'} 
    else if(x == 11){out <- 'eleven'} 
    else if(x == 12){out <- 'twelve'} 
    else if(x == 13){out <- 'thirteen'} 
    else if(x == 14){out <- 'fourteen'} 
    else if(x == 15){out <- 'fifteen'} 
    else if(x == 16){out <- 'sixteen'} 
    else if(x == 17){out <- 'seventeen'} 
    else if(x == 18){out <- 'eighteen'} 
    else if(x == 19){out <- 'nineteen'} 
    else if(x == 20){out <- 'twenty'} 
    else if(x >  20){out <- x}
  }
  return(out)
}

firstUp <- function(x){
  # From https://stackoverflow.com/questions/18509527/first-letter-to-upper-case
   substr(x, 1, 1) <- toupper(substr(x, 1, 1))
x
}

sigStars <- function(x){
  # Adding stars to indicate significance
  stars <- rep("", length(x))
  stars[x <= 0.1   & x > 0.05]   <- '.' # trend
  stars[x <= 0.05  & x > 0.01]   <- '*'
  stars[x <= 0.01  & x > 0.001]  <- '**'
  stars[x <= 0.001]              <- '***'
  return(stars)
}

createResultTable <- function(x){
  # Creating a nice looking table
  if(inherits(x, "glmerMod")){
    # For glmer table
    xTable        <- summary(x)$coefficients
    xTable        <- data.frame(xTable)
    xTable[, 1]   <- round(xTable[, 1], 2)
    xTable[, 2]   <- round(xTable[, 2], 2)
    xTable[, 3]   <- round(xTable[, 3], 2)
    xTable[, 4]   <- round(xTable[, 4], 4)
    xTable        <- cbind(xTable, sigStars(xTable[, 4]))
    names(xTable) <- c('Estimate', 'SE', 'Z', 'P', 'Sig')
  } else if(inherits(x, 'lmerModLmerTest')){
    xTable        <- summary(x)$coefficients
    xTable        <- data.frame(xTable)
    xTable[, 1]   <- round(xTable[, 1], 2)
    xTable[, 2]   <- round(xTable[, 2], 2)
    xTable[, 3]   <- round(xTable[, 3], 2)
    xTable[, 4]   <- round(xTable[, 4], 2)
    xTable[, 5]   <- round(xTable[, 5], 4)
    xTable        <- cbind(xTable, sigStars(xTable[, 5]))
    names(xTable) <- c('Estimate', 'SE', 'DF', 'T', 'P', 'Sig')
  } else if(inherits(x, 'anova')){
    if(attributes(x)$heading == "Analysis of Variance Table of type III  with  Satterthwaite \napproximation for degrees of freedom"){
      # Only ANOVA on lmerTest models with Satterthwaite approximation
      xTable        <- data.frame(x)
      xTable[, 1]   <- round(xTable[, 1], 2)
      xTable[, 2]   <- round(xTable[, 2], 2)
      xTable[, 3]   <- round(xTable[, 3], 2)
      xTable[, 4]   <- round(xTable[, 4], 2)
      xTable[, 5]   <- round(xTable[, 5], 2)
      xTable[, 6]   <- round(xTable[, 6], 4)
      xTable        <- cbind(xTable, sigStars(xTable[, 6]))
      names(xTable) <- c('SS', 'MSS', 'nDF', 'dDF', 'F', 'P', 'Sig') 
    } else {
      xTable <- data.frame('######', 'No known model', '######')
      names(xTable) <- c('%%%', '***', '&&&')
    }
  } else {
    xTable <- data.frame('######', 'No known model', '######')
    names(xTable) <- c('%%%', '***', '&&&')
  }
  return(xTable)
}
```

# Loading data
```{r}
# Data from experiment 1
load("data/exp1Data.RData")
dataSchemaVR1 <- combData
rm(combData)

#Add (non-)kitchen object factor to data
dataSchemaVR1$expectedInKitchen                                      <- 'non-kitchen'
dataSchemaVR1[which(dataSchemaVR1$objNum < 13), 'expectedInKitchen'] <- 'kitchen'

# Notes and decision on participants:
# Participant #1: Sligthly different instructions for no-memory trial -> exlcude.
# Participant #1 to #6: Had no familiarisation phase
# Participant #5 and # 15: Second exposure with stimulus -> include.
# Participant #9: No memory trials on trial 1 and 2 because I pressed those accidently -> changed.
dataSchemaVR1[which(dataSchemaVR1$subNum == 9 & (dataSchemaVR1$recallTrial == 1 | dataSchemaVR1$recallTrial == 2)), 'recallNoMemory'] <- 1
# Participant #10: Problems with data and from towel -> no anormalities.
# Participant #13: Had a longer delay (1.5 min)
dataSchemaVR1 <- subset(dataSchemaVR1, subNum != 1)


# Data from experiment 2
load("data/exp2Data.RData")
dataSchemaVR2 <- combData
rm(combData)

#Add (non-)kitchen object factor to data
dataSchemaVR2$expectedInKitchen                                      <- 'non-kitchen'
dataSchemaVR2[which(dataSchemaVR2$objNum < 13), 'expectedInKitchen'] <- 'kitchen'

# Notes and decision on participants:
# Participant #20 to #24: Exclude participants because they did the  wrong objLocTargetRating
dataSchemaVR2 <- subset(dataSchemaVR2, subNum >= 25)
# Participant #20 to #27: Foil2 for umbrella was not saved.
# Participant #25 to #27: Rated object 16 at location 1 instead of 14. This value is therefore missing.
# Participant #22: Seen objects twice but is excluded anyway. 
# Participant #26: Check recall microwave because it was correct (it is). The 2nd rating was 100 not 0.
dataSchemaVR2[which(dataSchemaVR2$subNum == 26 & dataSchemaVR2$objNum == 3), 'generalRatingPost'] <- 100
```

# Testing U-shape for kitchen objects only
The general expectancy of objects, i.e. how expected are these objects in general for the kitchen, significantly predicted memory in the first experiment (schemaVR1). In order to rule out that this confounded our conclusion, I only analyse kitchen objects while merging the two data sets. 

```{r}
# Using only trials with item memory
dataSchemaVR1_recall <- subset(dataSchemaVR1, dataSchemaVR1$recallNoMemory == 0)
dataSchemaVR1_AFC    <- subset(dataSchemaVR1, dataSchemaVR1$resCon != 1)
dataSchemaVR2_AFC    <- subset(dataSchemaVR2, dataSchemaVR2$resCon != 0)
dataSchemaVR2_recall <- subset(dataSchemaVR2, dataSchemaVR2$recallMemory != 0)

# Combining both datasets
# For Recall data
combinedDataRecall <- data.frame(subNum = c(dataSchemaVR1_recall$subNum, 
                                            dataSchemaVR2_recall$subNum),
                           accRecall = c(dataSchemaVR1_recall$accRecall, 
                                         dataSchemaVR2_recall$accRecall),
                           euclideanDist = c(dataSchemaVR1_recall$euclideanDist, 
                                             dataSchemaVR2_recall$euclideanDist),
                           accAFC = c(dataSchemaVR1_recall$accAFC, dataSchemaVR2_recall$accAFC),
                           objLocTargetRating = c(dataSchemaVR1_recall$objLocTargetRating, 
                                                  dataSchemaVR2_recall$objLocTargetRating),
                           generalRatingPost = c(dataSchemaVR1_recall$generalRatingPost, 
                                                 dataSchemaVR2_recall$generalRatingPost),
                           expectedInKitchen = c(dataSchemaVR1_recall$expectedInKitchen, 
                                                 dataSchemaVR2_recall$expectedInKitchen))
  
combinedDataRecall$objLocTargetRating <- scale(combinedDataRecall$objLocTargetRating)
combinedDataRecall$generalRatingPost  <- scale(combinedDataRecall$generalRatingPost)
combinedDataRecallKitchen             <- subset(combinedDataRecall, combinedDataRecall$expectedInKitchen == 'kitchen')

# For 3AFC
combinedDataAFC <- data.frame(subNum = c(dataSchemaVR1_AFC$subNum, dataSchemaVR2_AFC$subNum),
                           accRecall = c(dataSchemaVR1_AFC$accRecall, 
                                         dataSchemaVR2_AFC$accRecall),
                           euclideanDist = c(dataSchemaVR1_AFC$euclideanDist, 
                                             dataSchemaVR2_AFC$euclideanDist),
                           accAFC = c(dataSchemaVR1_AFC$accAFC, dataSchemaVR2_AFC$accAFC),
                           objLocTargetRating = c(dataSchemaVR1_AFC$objLocTargetRating, 
                                                  dataSchemaVR2_AFC$objLocTargetRating),
                           generalRatingPost = c(dataSchemaVR1_AFC$generalRatingPost, 
                                                 dataSchemaVR2_AFC$generalRatingPost),
                           expectedInKitchen = c(dataSchemaVR1_AFC$expectedInKitchen, 
                                                 dataSchemaVR2_AFC$expectedInKitchen))
  
combinedDataAFC$objLocTargetRating <- scale(combinedDataAFC$objLocTargetRating)
combinedDataAFC$generalRatingPost  <- scale(combinedDataAFC$generalRatingPost)
combinedDataAFCKitchen             <- subset(combinedDataAFC, combinedDataAFC$expectedInKitchen == 'kitchen')

# Models
# Accuracy recall
model1 <- glmer(accRecall ~  objLocTargetRating +
                I(objLocTargetRating*objLocTargetRating) + 
                (1 | subNum), 
              data = combinedDataRecallKitchen,
              family = binomial,
              control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 1)

kable(createResultTable(model1))

# Euclidean distance (Euclidean error)
model2 <- lmer(euclideanDist ~  objLocTargetRating +
               I(objLocTargetRating*objLocTargetRating) +
               (1 | subNum),
             data = combinedDataRecallKitchen)
kable(createResultTable(model2))

# Accuracy 3AFC
model3 <- glmer(accAFC ~  objLocTargetRating +
                I(objLocTargetRating*objLocTargetRating) + 
                (1 | subNum), 
              data = combinedDataRecallKitchen,
              family = binomial,
              control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 1)

kable(createResultTable(model3))

```

The U-shaped is significant for recall accucracy, Euclidean error and 3AFC accuracy. Thus, the interpretation is not different from the initial analysis and there is no need to find eight kitchen objects to replace the non-kitchen objects, which is something we thought about (see blog post).


# Analyse r/k judgement with independence scoring
In the pre-registration, we specified to try do disentablge the incongruency from the congruency advantage using a median-split based on the object/location expectancy and hoped to find that the percentage of familiar is higher for expected locations and vice versa for incogruent locations.

One shortcoming for the previous analysis layed in the scoring of familiarity.  Everytime someone recollects something, this person also experiences familiarity, hence I now use independence scoring to calculate familiarity. 

$Familiarity = \frac{p(familiar)}{1 - p(recollect)}$

I re-run the anaylsis with this newly calculated measure. Furthermore, we realised that the previosuly proposed analysis might not be the most adaequate to show that relationship. Therefore, I also reported a correlational analysis in my first year report. In complement that analsysis, I want to compare whether the correlation between object/location expectancy and number of recollect and familiar responses for each objects are significantly different. I use William's test for that, which can be used to compare two dependent correlation coefficients. 


```{r}
# Aggregating for each object
schemaVR2_table <- ddply(dataSchemaVR2, 
                          c('objNum', 'objName'), 
                          summarise,
                          objLocTargetRating = mean(objLocTargetRating, na.rm = TRUE),
                          RecallNoMemory     = table(recallMemory)[1],
                          RecallRecollection = table(recallMemory)[2],
                          RecallFamiliarity  = table(recallMemory)[3],
                          RecallGuess        = table(recallMemory)[4],
                          RecallN            = sum(table(recallMemory)),
                          RecallRec_per      = (RecallRecollection/RecallN),
                          RecallFam_per      = (RecallFamiliarity/RecallN),
                          RecallFam_indepen  = RecallFam_per/(1-RecallRec_per),
                          AFCNoMemory        = table(resCon)[1], 
                          AFCRecollection    = table(resCon)[2],
                          AFCFamiliarity     = table(resCon)[3],
                          AFCGuess           = table(resCon)[4],
                          AFCN               = sum(table(resCon)),
                          AFCRec_per         = (AFCRecollection/AFCN),
                          AFCFam_per         = (AFCFamiliarity/AFCN),
                          AFCFam_indepen     = AFCFam_per/(1-AFCRec_per))

# Median split
objLoc_expectedness <- rep('expected',dim(schemaVR2_table)[1])
objLoc_expectedness[which(schemaVR2_table$objLocTargetRating <= median(schemaVR2_table$objLocTargetRating))] <- 'unexpected'
schemaVR2_table$objLoc_expectedness <- objLoc_expectedness


# Familiarity
t.test(RecallFam_indepen ~ objLoc_expectedness, schemaVR2_table, var.equal = TRUE)
t.test(AFCFam_indepen ~ objLoc_expectedness, schemaVR2_table, var.equal = TRUE)
#cor.test( ~ RecallFam_indepen + objLocTargetRating, schemaVR2_table)
#cor.test( ~ AFCFam_indepen + objLocTargetRating, schemaVR2_table)
#cor.test( ~ RecallFam_per + objLocTargetRating, schemaVR2_table)
cor_recExp_AFC    <- cor.test( ~ AFCRec_per + objLocTargetRating, schemaVR2_table)
cor_famExp_AFC    <- cor.test( ~ AFCFam_per + objLocTargetRating, schemaVR2_table)
cor_famRec_AFC    <- cor.test( ~ AFCFam_per + AFCRec_per, schemaVR2_table)
cor_famIndExp_AFC <- cor.test( ~ AFCFam_indepen + objLocTargetRating, schemaVR2_table)
cor_famIndRec_AFC <- cor.test( ~ AFCFam_indepen + AFCRec_per, schemaVR2_table)

# William's test
r.test(n = 20,
       r12 = cor_recExp_AFC$estimate,
       r13 = cor_famExp_AFC$estimate,
       r23 = cor_famRec_AFC$estimate)

r.test(n = 20,
       r12 = cor_recExp_AFC$estimate,
       r13 = cor_famIndExp_AFC$estimate,
       r23 = cor_famIndRec_AFC$estimate)

```

Independence scoring do not provide additional insight but it reduces the correlation between object/location expectancy and familiarity. However, the correlation coefficients are significantly different providing evidence that the memory advantage at either end of the scale arises relies on different processes. 

## Logistic regression for r/k judgements
In another attempt to refine our analysis, I try to predict recollect and familiar response in a mixed linear model similar to those used to predict memory performance. 

```{r}
# Coding data
# Recall
dataSchemaVR2$recallRecollection <- 0
dataSchemaVR2$recallRecollection[which(dataSchemaVR2$recallMemory == 1)] <- 1
dataSchemaVR2$recallRecollection <- factor(dataSchemaVR2$recallRecollection,
                                           levels = c(0, 1),
                                           labels = c('rec', 'no rec'))

dataSchemaVR2$recallFamiliarity <- 0
dataSchemaVR2$recallFamiliarity[which(dataSchemaVR2$recallMemory == 2)] <- 1
dataSchemaVR2$recallFamiliarity <- factor(dataSchemaVR2$recallFamiliarity,
                                           levels = c(0, 1),
                                           labels = c('fam', 'no fam'))

# 3AFC
dataSchemaVR2$AFCRecollection <- 0
dataSchemaVR2$AFCRecollection[which(dataSchemaVR2$resCon == 1)] <- 1
dataSchemaVR2$AFCRecollection <- factor(dataSchemaVR2$AFCRecollection,
                                           levels = c(0, 1),
                                           labels = c('rec', 'no rec'))

dataSchemaVR2$AFCFamiliarity <- 0
dataSchemaVR2$AFCFamiliarity[which(dataSchemaVR2$resCon == 2)] <- 1
dataSchemaVR2$AFCFamiliarity <- factor(dataSchemaVR2$AFCFamiliarity,
                                           levels = c(0, 1),
                                           labels = c('fam', 'no fam'))


# Models
model4 <-glmer(recallRecollection ~  objLocTargetRating  +
                (1 | subNum), 
              data = dataSchemaVR2,
              family = binomial,
              control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 1)

kable(createResultTable(model4))

model5 <- glmer(AFCRecollection ~  objLocTargetRating  +
                (1 | subNum), 
              data = dataSchemaVR2,
              family = binomial,
              control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 1)

kable(createResultTable(model5))

model6 <- glmer(recallFamiliarity ~  objLocTargetRating  +
                (1 | subNum), 
              data = dataSchemaVR2,
              family = binomial,
              control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 1)

kable(createResultTable(model6))

model7 <- glmer(AFCFamiliarity ~  objLocTargetRating  +
                (1 | subNum), 
              data = dataSchemaVR2,
              family = binomial,
              control = glmerControl(optimizer = "bobyqa"),
              nAGQ = 1)

kable(createResultTable(model7))
```

This analysis does not converge with the corrrelational analysis. In contrast, object/location expectancy show a positive relationship with recollection. The reason of this discrepancy is not clear.