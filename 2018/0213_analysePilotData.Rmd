---
title: "Analyse pilot data"
author: "JÃ¶rn Alexander Quent"
date: "12 February 2018"
output:
  html_document:
    toc: true
    toc_depth: 6
    theme: united
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
options(scipen = 30)
```

## Introduction
This script is about to check whether the parameters of the experiment (such as time in the VE, number of targets objects etc.) work to avoid either floor or ceiling effects. Note that the pre-encoding ratings (see below) were from different individuals. 

## Preliminary stuff
### Libraries & functions
```{r}
library(data.table)
library(plyr)
library(ggplot2)
library(lme4)
library(ez)
library(BayesFactor)
library(grid)

pValue <-function(x, sign = '='){
  if (inherits(x, "lm")){
    s <- summary.lm(x)
    x <- pf(s$fstatistic[1L], s$fstatistic[2L], s$fstatistic[3L], lower.tail = FALSE)
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  } else {
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  }
  return(x.converted)
}

rValue <-function(x){
  if (inherits(x, "lm")){
    r.squared <- summary(x)$r.squared
    x.converted <- paste('=',substr(as.character(round(r.squared, 3)), 2,5)) 
  } else {
    if (x < 0){
      x.converted <- paste('= -',substr(as.character(abs(round(x, 3))), 2,5), sep = '') 
    } else {
      x.converted <- paste('=',substr(as.character(abs(round(x, 3))), 2,5)) 
    }
  }
  return(x.converted) 
}
```

### Loading data
```{r}
# Using fread instead of read.table because it is faster but rbindlist(lapply(fileNames, read.table)) would also work. This is my attempt to use more vectorisation in my code.
population        <- 2:6
fileNames         <- paste('data/retrievalTask_', as.character(population),'.dat', sep = '')
dataRetrievalTask <- rbindlist(lapply(fileNames, fread))

# Variable and objects names
names(dataRetrievalTask) <- c('subNo', 'date', 'time', 'trial','objNum', 'targetLocation', 'targetRankPre', 'foil1Location', 'foil1RankPre', 'foil2Location', 'foil2RankPre', 'AFCPreTime','rtAFC', 'resAFC', 'accAFC', 'conPreTime', 'rtCon', 'resCon', 'left', 'middle', 'right')
objNames              <- c('microwave','kitchen roll','saucepan', 'toaster','bowl of fruits','tea pot','knife','mixer','bread','glass jug','mug','dishes','towels','toy','pile of books','umbrella','hat','helmet','calendar','fan')

# Assign factor labels
dataRetrievalTask$objName <- factor(dataRetrievalTask$objNum, labels = objNames)
dataRetrievalTask$left       <- factor(dataRetrievalTask$left, labels = c('target', 'foil1', 'foil2'))
dataRetrievalTask$middle     <- factor(dataRetrievalTask$middle, labels = c('target', 'foil1', 'foil2'))
dataRetrievalTask$right      <- factor(dataRetrievalTask$left, labels = c('target', 'foil1', 'foil2'))
dataRetrievalTask$resCon     <- as.factor(dataRetrievalTask$resCon)
```

## Confidence decision
```{r}
# Legend: 1 = Did not see object| 2 = Guess the object was there | 3 = Know the object was there
table1 <- table(dataRetrievalTask$resCon)
```
After indicating which location was correct, participants indicated their confidence. There were `r table1[1]` 'Did not see object', `r table1[2]` 'Guess the object was there' and `r table1[2]` 'Know the object was there' responses in total. 

Another way to look at the data is to check the confidence responses aggregated for each participant and each object. The latter is especially important to see whether a object was repeatedly missed. 
```{r}
# Looking at confidence response for each participant
conSubNo <- ddply(dataRetrievalTask, c('subNo'), summarise, notSeen = table(resCon)[1], guessed = table(resCon)[2], knew = table(resCon)[3])
conSubNo

# Looking at confidence responses for each object
conobjName <- ddply(dataRetrievalTask, c('objName'), summarise, notSeen = table(resCon)[1], guessed = table(resCon)[2], knew = table(resCon)[3])
conobjName
```
 
As can be seen above, participants didn't see the knife in `r conobjName[7, 2]` out of `r conobjName[7, 2] + conobjName[7, 3] + conobjName[7, 4]` times. This is clearly problematic.  Interestingly, `r conobjName[1, 2]` participants indicated that they did not see microwave even though it is the biggest movable object in the room. I should probably exclude the knife from the analysis as already noted [here](https://jaquent.me/2018/01/25/196/) because both foils are to similar as they are all on the working surface. Additionally, it is possible that the knife is just not visible enough. The other object look fine. 

## Accuracy for AFC decision (task performance)
```{r}
# For each participant
dataRetrievalTaskSub1 <- subset(dataRetrievalTask, dataRetrievalTask$resCon != 1 & dataRetrievalTask$objNum != 7)
agg2                  <- ddply(dataRetrievalTaskSub1, c('subNo'), summarise, accAFC = mean(accAFC))
agg2
```

If one ignores the knife and all trials, on which participants indicated that they didn't see the object, the average accuracy is `r round(mean(dataRetrievalTaskSub1$accAFC), 2)`.

```{r}
ggplot(agg2, aes(x = 1 , y = accAFC)) + 
  geom_jitter(width = 0.1, height = 0, alpha = 0.5) + 
  geom_hline(yintercept = 1/3) + 
  geom_segment(aes(x = 0.95, y = mean(agg2$accAFC), xend = 1.05, yend = mean(agg2$accAFC))) +
  annotate('text', x = 0.95, y = (1/3) - 0.03, label = 'Chance (1/3)') + 
  labs(y = 'Accuracy (3AFC)', x = '') + 
  coord_cartesian(ylim = c(0, 1), xlim = c(0.9, 1.1)) + 
  theme(plot.margin = margin(10, 10, 10, 10))
```

The average performance seems to be in good range. The highest performance is `r round(max(agg2$accAFC), 2)`, which is likely to be a bit inflated because this participant had longer exposure time. The lowest value `r round(min(agg2$accAFC), 2)` is still considerably above chance. The general difficulty seems to be appropriate. 

## Ratings
```{r}
# This chunk is about combining the data from the retrieval part with the data with the post-encoding ratings. 
# Loading data and adding objects names
fileNames         <- paste('data/ratingTask_', as.character(population),'.dat', sep = '')
dataRatingTask    <- rbindlist(lapply(fileNames, fread))

names(dataRatingTask) <- c('subNo', 'date', 'time', 'trial','objNum', 'location', 'rating', 'RT')

objNames <- c('microwave','kitchen roll','saucepan', 'toaster','bowl of fruits','tea pot','knife','mixer','bread','glass jug','mug','dishes','towels','toy','pile of books','umbrella','hat','helmet','calendar','fan')

dataRatingTask$objName <- factor(dataRatingTask$objNum, labels = objNames)


# Collapsing acrossp participants as well as objects and splitting location and rating values into three separate values
dataRatingTask_agg <- ddply(dataRatingTask, c('subNo', 'objName', 'objNum'), summarise, 
                            location1 = location[1],  
                            location2 = location[2],  
                            location3 = location[3],
                            rating1   = rating[1],
                            rating2   = rating[2],
                            rating3   = rating[3])

# Basically I need the following bit to sort the data set
dataRetrievalTask_agg <- ddply(dataRetrievalTask, c('subNo', 'objName', 'objNum'), summarise, 
                               foil1Location  = foil1Location, 
                               targetLocation = targetLocation, 
                               foil2Location  = foil2Location)

# Finding the respective ratings for targets and foils because this information is not saved for the ratingTask data set
# Target
postRatingTarget  <- rep(NA, 20)
postRatingTarget[which(dataRatingTask_agg$location1 == dataRetrievalTask_agg$targetLocation)] <- dataRatingTask_agg$rating1[which(dataRatingTask_agg$location1 == dataRetrievalTask_agg$targetLocation)]
postRatingTarget[which(dataRatingTask_agg$location2 == dataRetrievalTask_agg$targetLocation)] <- dataRatingTask_agg$rating2[which(dataRatingTask_agg$location2 == dataRetrievalTask_agg$targetLocation)]
postRatingTarget[which(dataRatingTask_agg$location3 == dataRetrievalTask_agg$targetLocation)] <- dataRatingTask_agg$rating3[which(dataRatingTask_agg$location3 == dataRetrievalTask_agg$targetLocation)]

# Foil 1
postRatingFoil1  <- rep(NA, 20)
postRatingFoil1[which(dataRatingTask_agg$location1 == dataRetrievalTask_agg$foil1Location)] <- dataRatingTask_agg$rating1[which(dataRatingTask_agg$location1 == dataRetrievalTask_agg$foil1Location)]
postRatingFoil1[which(dataRatingTask_agg$location2 == dataRetrievalTask_agg$foil1Location)] <- dataRatingTask_agg$rating2[which(dataRatingTask_agg$location2 == dataRetrievalTask_agg$foil1Location)]
postRatingFoil1[which(dataRatingTask_agg$location3 == dataRetrievalTask_agg$foil1Location)] <- dataRatingTask_agg$rating3[which(dataRatingTask_agg$location3 == dataRetrievalTask_agg$foil1Location)]

# Foil 2
postRatingFoil2  <- rep(NA, 20)
postRatingFoil2[which(dataRatingTask_agg$location1 == dataRetrievalTask_agg$foil2Location)] <- dataRatingTask_agg$rating1[which(dataRatingTask_agg$location1 == dataRetrievalTask_agg$foil2Location)]
postRatingFoil2[which(dataRatingTask_agg$location2 == dataRetrievalTask_agg$foil2Location)] <- dataRatingTask_agg$rating2[which(dataRatingTask_agg$location2 == dataRetrievalTask_agg$foil2Location)]
postRatingFoil2[which(dataRatingTask_agg$location3 == dataRetrievalTask_agg$foil2Location)] <- dataRatingTask_agg$rating3[which(dataRatingTask_agg$location3 == dataRetrievalTask_agg$foil2Location)]

# Creating new combined data set and exclude guess responses
combData                  <- dataRetrievalTask
combData$postRatingTarget <- postRatingTarget
combData$postRatingFoil1  <- postRatingFoil1
combData$postRatingFoil2  <- postRatingFoil2
combData[which(combData$objNum == 7),]
# Again exluding all didn't see repsonses and the knife. 
combData                  <- subset(combData, combData$resCon != 1 & combData$objNum != 7)
```

## object level
### Relationship between expectancy and 3AFC performance
```{r}
combDataAfcExp  <- ddply(combData, c('objName', 'objNum'), summarise, afc = mean(accAFC), preRank = mean(targetRankPre), postRating = mean(postRatingTarget))

# How well do the pre-ecnoding ranks correlate with the post-encoding ratings.
corrPrePost <- cor.test(combDataAfcExp$preRank, combDataAfcExp$postRating)
```

It might be interesting to note that the correlation between the pre-encoding ranks and post-encoding ratings is *r* `r rValue(corrPrePost$estimate)`, *p* `r pValue(corrPrePost$p.value)`. This clearly shows that ratings differ if you rated other combinations of objects and locations as well. 

```{r}
plot1 <- ggplot(combDataAfcExp, aes(x = postRating, y = afc)) + 
  geom_point() +
  geom_text(aes(label = objNum),hjust = 0, vjust = 0) + 
  geom_smooth() +  
  labs(y = 'Mean accuracy (3AFC)', x = "Expectancy", title = 'Relationship with post-encoding ratings') + 
  coord_cartesian(ylim = c(0, 1.5), xlim = c(-100, 100), expand = FALSE) + 
  theme(plot.margin = unit(c(1,7,1,1), "lines"))
for (i in 1:dim(combDataAfcExp)[1])  {
  height <- seq(0.2, 1.3, length = dim(combDataAfcExp)[1])
plot1 <- plot1 + annotation_custom(
      grob = textGrob(label = paste(as.character(combDataAfcExp$objNum[i]), "=", combDataAfcExp$objName[i]), hjust = 0),
      ymin = height[i],      # Vertical position of the textGrob
      ymax = height[i],
      xmin = 102,         # Note: The grobs are positioned outside the plot area
      xmax = 102)
 }   

gt <- ggplot_gtable(ggplot_build(plot1))
gt$layout$clip[gt$layout$name == "panel"] <- "off"
grid.draw(gt)
```

```{r}
plot2 <- ggplot(combDataAfcExp, aes(x = preRank, y = afc)) + 
  geom_point() +
  geom_text(aes(label = objNum),hjust = 0, vjust = 0) + 
  geom_smooth() +  
  labs(y = 'Mean accuracy (3AFC)', x = "Expectancy", title = 'Relationship with pre-encoding ranks') +  
  coord_cartesian(ylim = c(0, 1.5), xlim = c(0, 400), expand = FALSE) + 
  theme(plot.margin = unit(c(1,7,1,1), "lines"))
for(i in 1:dim(combDataAfcExp)[1]) {
  height <- seq(0.2, 1.3, length = dim(combDataAfcExp)[1])
plot2 <- plot2 + annotation_custom(
      grob = textGrob(label = paste(as.character(combDataAfcExp$objNum[i]), "=", combDataAfcExp$objName[i]), hjust = 0),
      ymin = height[i],      # Vertical position of the textGrob
      ymax = height[i],
      xmin = 402,         # Note: The grobs are positioned outside the plot area
      xmax = 402)
 }   

gt <- ggplot_gtable(ggplot_build(plot2))
gt$layout$clip[gt$layout$name == "panel"] <- "off"
grid.draw(gt)
```

If the knife were included in this analysis, it would be an outlier completely changing the smoothing curve. However both plots show that there might be no memory benefit for expected object when it comes to object/location memory, but there seems benefit for objects at unexpected locations. That is very interesting because there is basically no relationship between both ratings. However the number of participants is far to small too make any real claims about the relationship.  

I think there could be a power problem because no matter how many participants I collect the analysis will be still run with 19 data points (20 minus the knife).

## Participant level
### Factorisation
Another way to look at the data is to split the data into three equally sized conditions based on the expectancy (i.e. unexpected, neutral, expected) for each participant. The problem however is that you cannot create equally sized conditions if the number of objects is twenty. The unexpected condition will hence comprise the lowest seven values, the neutral condition the next six values and the expected condition the next seven values. 
```{r}
# Based on post-encoding ratings
combData2 <- combData
combData2$postRatingTargetRank <- ddply(combData2, c('subNo'), summarise, postRatingTarget = postRatingTarget, rank = rank(postRatingTarget))$rank

combData2$expFact <- 2
combData2$expFact[which(combData2$postRatingTargetRank <= 7)] <- 1
combData2$expFact[which(combData2$postRatingTargetRank >= 14)] <- 3
combData2$expFact <- factor(combData2$expFact, labels = c("unexpected", "neutral", "expected"))

combData2Exp <- ddply(combData2, c('subNo', 'expFact'), summarise, 
                     N = length(accAFC),  
                     mean = mean(accAFC),
                     ratingsMean = mean(postRatingTarget))

# It is important to aggregate data, which was already aggregated.
combData2ExpAgg <- ddply(combData2Exp, c('expFact'), summarise, 
                        N = length(mean),  
                        accAfcMean = mean(mean), 
                        accAfcSd = sd(mean), 
                        accAfcSe = accAfcSd/sqrt(N))

ggplot(combData2Exp, aes(x = expFact, y = mean)) +
  geom_jitter(width = 0.2, height = 0, alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "line", aes(group=1))  +
  labs(y = 'Mean accuracy (3AFC)', x = "Expectancy", title = 'Relationship with post-encoding ratings') + 
  theme(plot.margin = margin(10, 10, 10, 10))
```

```{r}
# Based on post-encoding ratings
combData3               <- combData
combData3$targetRankPre <- ddply(combData3, c('subNo'), summarise, targetRankPre = targetRankPre, rank = rank(targetRankPre))$rank

combData3$expFact <- 2
combData3$expFact[which(combData3$targetRankPre <= 7)] <- 1
combData3$expFact[which(combData3$targetRankPre >= 14)] <- 3
combData3$expFact <- factor(combData3$expFact, labels = c("unexpected", "neutral", "expected"))

combData3Exp <- ddply(combData3, c('subNo', 'expFact'), summarise, 
                     N = length(accAFC),  
                     mean = mean(accAFC),
                     ratingsMean = mean(targetRankPre))

# It is important to aggregate data, which was already aggregated.
combData3ExpAgg <- ddply(combData3Exp, c('expFact'), summarise, 
                        N = length(mean),  
                        accAfcMean = mean(mean), 
                        accAfcSd = sd(mean), 
                        accAfcSe = accAfcSd/sqrt(N))

ggplot(combData3Exp, aes(x = expFact, y = mean)) +
  geom_jitter(width = 0.2, height = 0, alpha = 0.5) +
  stat_summary(fun.y = mean, geom = "line", aes(group=1))  +
  labs(y = 'Mean accuracy (3AFC)', x = "Expectancy", title = 'Relationship with pre-encoding ranks') + 
  theme(plot.margin = margin(10, 10, 10, 10))
```

## Conclusion
Overall, it looks like the paradigm could work to inform me about the relationship between schema-congruency and object/location memory. The performance is in a good range. Furthermore, it became apparent that I might need exclude the knife from the analysis after looking at the confidence responses this item received. Therefore I will exclude it if this pattern persist in data, which I will begin to collect. The next step will be to write the real analysis script with pre-processing and so on and formulated hypotheses. After that and fixing some issue in the protocol, I will start collecting data. Yay :)