---
title: "Stimulus selection: 3"
author: "JÃ¶rn Alexander Quent"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 6
    theme: united
---
```{r setup, include=FALSE}
knitr::opts_knit$set(eval.after = 'fig.cap')
knitr::opts_chunk$set(echo = TRUE, 
                      warning = FALSE, 
                      message = FALSE,
                      fig.width = 10, 
                      fig.height= 7, 
                      dpi=300, 
                      out.width="1200px", 
                      out.height="700px")
options(scipen = 30)
```

# Introduction
This script details how I select the location for all objects for my second experiment of [project 1](https://jaquent.me/projects/) (schemaVR2). This script is a slight adaption of the [script](https://jaquent.github.io/2018/0125_stimulusSelection2.html) that I used to select the locations for my first experiment (schemaVR1). The second experiment is carried out because no objects that are generally highly expected in the kitchen (object one to twelve) was in the middle of object/location expectancy scale. It was therefore impossible to disentangle the effect of object/location expectancy on memory from general expectancy. The latter was clearly negatively correlated with memory performance in schemaVR1. Interim results can be found [here](https://jaquent.github.io/2018/0424_interimResults.html). For this time, I changed the algorithm so that only the sum squared differences for target location of those objects that are expected in the kitchen were used for selection. 

# Used libraries and functions
```{r}
library(ggplot2)
library(ez)
library(tidyr)
library(knitr)
library(gridExtra)
library(grid)

pValue <-function(x, sign = '='){
  if (inherits(x, "lm")){
    s <- summary.lm(x)
    x <- pf(s$fstatistic[1L], s$fstatistic[2L], s$fstatistic[3L], lower.tail = FALSE)
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  } else {
    if(x > 1){
      stop("There is no p-value greater than 1")
    } else if(x < 0.001){
      x.converted <- '< .001'
    } else{
      x.converted <- paste(sign,substr(as.character(round(x, 3)), 2,5))
    } 
  }
  return(x.converted)
}
```


# Sampling the distributions
In this attempt, I randomly select locations for the targets and for foil 1 as well as foil 2 with *sample()* without replacement. After that I make sure that same location is not used for foil 1, foil 2 and/or the target by repeating the step until it is not the case anymore. I then calculate the sum of squared differences between the ranks of the targets and the intended spread (i.e. *targetSpread*; SS of targets). Note here that I only use object one to twelve here, which are the objects which generally highly expected in the kitchen. The sum of squared differences between the ranks of the targets and both foils (SS of foils) was also calculated. Both SSs will be used later to select a set in two-step process. 

Because the number of combinations is high, I used the CBU's computer cluster (2.4 GHz CPU with 28 cores and 256 GB RAM). 

```{r , eval = FALSE}
# Functions
progressDisplay <- function(i, iterations){
  cat('\rProgress: |',rep('=',floor((i/iterations)*50)),rep(' ',50 - floor((i/iterations)*50)),'|', sep = '')
}

# Preparing
subNo                <- 1:6
N                    <- length(subNo)
numberObjects        <- 20
kitchenOjects        <- 1:12
numberKitchenObjects <- length(kitchenOjects)
nonKitchenObjets     <- 13:30
locationRatings      <- array(data = NA, dim = c(numberObjects, numberObjects, N))

# Sequently loading data
for(i in 1:N){
  locationRatings[,,i] <- matrix(scan(paste('data/locationRatings_', as.character(subNo[i]) ,'.dat', sep = '')), byrow = TRUE, ncol = 20)
}

# Calculating ranks
rankedRatings <- array(data = NA, dim = c(numberObjects, numberObjects, N))

for(i in 1:N){
  rankedRatings[,,i] <- rank(locationRatings[,,i])
}

# Calculating mean rank per object/ location
meanRankedRatings <- apply(rankedRatings, 1:2, mean)

# Calculating the targetspread of the 12 kitchen objects
targetSpread <- seq(1, 400,length = numberKitchenObjects)

iterations <- 9999999
SS_target  <- rep(NA, iterations)
SS_foils   <- rep(NA, iterations)
targets    <- matrix(NA, nrow = numberObjects, ncol = iterations)
foils1     <- matrix(NA, nrow = numberObjects, ncol = iterations)
foils2     <- matrix(NA, nrow = numberObjects, ncol = iterations)

for(i in 1:iterations){
  targets[, i] <- sample(numberObjects)
  foils1[, i]  <- sample(numberObjects)
  foils2[, i]  <- sample(numberObjects)
  # Drawing anohter sample if same location is used more than once
  while(any(foils1[, i] == foils2[, i]) | any(targets[, i] == foils2[, i]) | any(targets[, i] == foils1[, i])){
    targets[, i] <- sample(numberObjects)
    foils1[, i]  <- sample(numberObjects)
    foils2[, i]  <- sample(numberObjects)
  }
  progressDisplay(i, iterations)

  # Calculating target SS based only on the 12 kitchen objects
  kitchenTargets <- targets[seq(kitchenOjects), i]
  SS_target[i]   <- sum((sort(meanRankedRatings[cbind(seq_along(kitchenTargets),  kitchenTargets)]) - targetSpread)^2)
  
  # Calculating foils SS based on all objects because every object needs similar foils
  SS_foils[i]    <- sum((meanRankedRatings[cbind(seq_along(targets[, i]),  targets[, i])] - meanRankedRatings[cbind(seq_along(foils1[, i]),  foils1[, i])])^2, (meanRankedRatings[cbind(seq_along(targets[, i]),  targets[, i])] - meanRankedRatings[cbind(seq_along(foils2[, i]),  foils2[, i])])^2)
  # For retrieval it is more imporant that foils are more similar than the targets spread is good enough
}

save.image(paste('data_twoFoils_',format(Sys.time(), "%Y%m%d_%H%M"), '.RData', sep = ""))

# Explanation:
# The aim of this script is to find the best combination of object/location and similar targets for the second 
# VR experiment (schemaVR2). The criterion is  that the spread ranked expectancy of the targets object at their 
# locaiton is equally spaced between 1 and 400. To do this, the script calculates the sum of squared differences 
# between a) the object/location expectancy of the selected locations for targets the  and the target spread and 
# b) the sum of squared differences between the object/location expectancy of the selected locations for targets and
# both foil locations.
```

```{r, echo = FALSE}
load("data_twoFoils_20180423_1607.RData")
```

# Selcting a set from the distributions
```{r}
# Selecting lowest values for further selection
cutOffDecimal <- 0.0001
cutOffValue   <- sort(SS_foils)[round(iterations * cutOffDecimal)]
sub_SS_foils  <- SS_foils[which(SS_foils <= cutOffValue)]
sub_SS_target <- SS_target[which(SS_foils <= cutOffValue)]

hist1 <- ggplot(data.frame(SS_foils), aes(SS_foils)) + 
  geom_histogram(binwidth = 100) + 
  geom_vline(xintercept = cutOffValue) + 
  theme(panel.margin = unit(2, "cm"), text = element_text(size = 12),  plot.margin = margin(10, 10, 10, 10)) + 
  labs(y = 'Number of occurences', x = 'Sum of squared differences', title = 'Distribution of SS of foils') + 
  coord_cartesian(expand = FALSE)

hist2 <- ggplot(data.frame(SS_target), aes(SS_target)) + 
  geom_histogram(binwidth = 100) + 
  theme(panel.margin = unit(2, "cm"), text = element_text(size = 12),  plot.margin = margin(10, 10, 10, 10)) + 
  labs(y = 'Number of occurences', x = 'Sum of squared differences', title = 'Distribution of SS of targets') + 
  coord_cartesian(expand = FALSE)

grid.arrange(hist1, hist2, ncol = 2)
```

It is more important that SS of foils is low because high SS of foils could bias participants to choose foils if the expectancy values systemically are higher/lower. To reduce this bias, I selected the `r cutOffDecimal`th percentile of the SS of foils. This cut off is shown in the histogram above on the left. In the next step, I plot the distributions of the SS of targets for this subset. 

```{r}
ggplot(data.frame(sub_SS_target), aes(sub_SS_target)) + 
  geom_histogram(binwidth = 100) + 
  theme(panel.margin = unit(2, "cm"), text = element_text(size = 12),  plot.margin = margin(10, 10, 10, 10)) + 
        
  labs(y = 'Number of occurences', x = 'Summed squared differences', title = 'Distribution of SS of targets (subset)') + 
  coord_cartesian(expand = FALSE)
```

Now, I sort the subset of `r length(sub_SS_target)` values that all have low SS of of foils values to select the set that has very low value in this subset. 

```{r}
objectNames <- c('microwave','kitchen roll','saucepan', 'toaster','fruit bowl','tea pot','knife','mixer','bread','glass jug','mug','dishes','towels','toy','pile of books','umbrella','hat','helmet','calendar','fan')

indexValue     <- sort(sub_SS_target)[3]
index          <- which(SS_target == indexValue)[1]
finalSelection <- data.frame(Object = objectNames, 
                             f1Rank = meanRankedRatings[cbind(seq_along(foils1[, index]), foils1[, index])],
                             tRank  = meanRankedRatings[cbind(seq_along(targets[, index]), targets[, index])],
                             f2Rank = meanRankedRatings[cbind(seq_along(foils2[, index]), foils2[, index])],
                             f1Loc  = foils1[, index],
                             tLoc   = targets[, index],
                             f2Loc  = foils2[, index])

finalSelection$diff1 <- finalSelection$tRank - finalSelection$f1Rank
finalSelection$diff2 <- finalSelection$tRank - finalSelection$f2Rank
kable(finalSelection[order(finalSelection$tRank),])
```

# Checking the set
Above you can see the final set. In a last step, I check that foils don't differ from targets in terms of the expectancy of their location. The last step will be to take this set to the VR in environment and check whether this configuration works; for instance whether it doesn't have any inappropriate combinations. If this would be the case, I could just pick one of the next sets. 

```{r}
# Converting data to long format
finalSelectionLong  <- gather(finalSelection, Type, Rank, tRank, f1Rank, f2Rank)
finalSelectionLong$expectedInKitchen <- 'high'
finalSelectionLong$expectedInKitchen[c(13:20, 33:40, 53:60)] <- 'low'


resultANOVA <- ezANOVA(data = finalSelectionLong, dv = .(Rank), wid = .(Object), within = .(Type), detailed = TRUE)
resultANOVA

ggplot(data = finalSelectionLong, aes(x = Type, y = Rank, color = expectedInKitchen)) + geom_jitter(width = 0.2)
```

Type (target vs. foil 1 vs. foil 2) has no significant effect on the ranks, *F*(`r resultANOVA$ANOVA[2,'DFn']`, `r resultANOVA$ANOVA[2,'DFd']`) = `r round(resultANOVA$ANOVA[2,'F'],2)`, *p* `r pValue(resultANOVA$ANOVA[2,'p'])`. The next step now is to load all in the VE and check if this set would work. 
