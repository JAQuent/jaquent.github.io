<!doctype html>
<html lang="en">
  <head>
    <meta charset="utf-8">
<meta http-equiv="x-ua-compatible" content="ie=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

<meta property="og:title" content="Comparing different methods to calculate Bayes factors for a simple mode" />
<meta property="og:description" content="Introduction Comparing the four BF methods Evidence for null hypothesis: brms vs. ttestBF() How much more conservative is the bmrs approach? Directed vs. non-directed hypothesis Conclusion  Introduction In this post, I’d like to examine how different ways to calcualte Bayes factors (BF) compare with each other for a simple model. This simulation was run on my department’s HPC using the rslurm package: the corresponding R code, data and ." />
<meta property="og:type" content="article" />
<meta property="og:url" content="/2020/07/comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode/" />
<meta property="article:published_time" content="2020-07-16T00:00:00+00:00" />
<meta property="article:modified_time" content="2020-07-16T00:00:00+00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Comparing different methods to calculate Bayes factors for a simple mode"/>
<meta name="twitter:description" content="Introduction Comparing the four BF methods Evidence for null hypothesis: brms vs. ttestBF() How much more conservative is the bmrs approach? Directed vs. non-directed hypothesis Conclusion  Introduction In this post, I’d like to examine how different ways to calcualte Bayes factors (BF) compare with each other for a simple model. This simulation was run on my department’s HPC using the rslurm package: the corresponding R code, data and ."/>



    <link rel="canonical" href="/2020/07/comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode/">

    <title>
      
        Comparing different methods to calculate Bayes factors for a simple mode | Jörn Alexander Quent&#39;s notebook
      
    </title>

    
    <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <link href="/css/style.css" rel="stylesheet">

    

    

    
  </head>
  <body>
    
      <header class="blog-header">
    <nav class="navbar navbar-expand-md navbar-light bg-light">
        <a class="navbar-brand" href="/">
            Jörn Alexander Quent&#39;s notebook
        </a>
        <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false"
            aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse justify-content-between" id="navbarNav">
            <ul class="navbar-nav">
                
                
                <li class="nav-item ">
                    
                        <a class="nav-link" href="/">Home</a>
                    
                </li>
                
                <li class="nav-item ">
                    
                        <a class="nav-link" href="/about/">About</a>
                    
                </li>
                
            </ul>
            
        </div>
    </nav>
</header>
    

    
    <div class="container">
      <div class="row">
        <div class="col-12 col-lg-8 blog-main">

          

<header>
    <h2 class="blog-post-title">
        <a class="text-dark" href="/2020/07/comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode/">Comparing different methods to calculate Bayes factors for a simple mode</a>
    </h2>
    


<div class="blog-post-date text-secondary">
    
        Jul 16, 2020
    
    
        by <span rel="author">Alex Quent</span>
    
</div>

    
<div class="blog-post-tags text-secondary">
    <strong>Tags:</strong>
    
        <a class="badge badge-primary" href="/tags/bayesian-stats">Bayesian Stats</a>
    
</div>

    
<div class="blog-post-categories text-secondary">
    <strong>Categories:</strong>
    
        <a class="badge badge-primary" href="/categories/general">general</a>
    
</div>

    <hr>
</header>
<article class="blog-post">
    <ul>
<li><a href="#introduction">Introduction</a></li>
<li><a href="#comparing-the-four-bf-methods">Comparing the four BF methods</a></li>
<li><a href="#evidence-for-null-hypothesis-brms-vs.-ttestbf">Evidence for null hypothesis: <code>brms</code>
vs. <code>ttestBF()</code></a></li>
<li><a href="#how-much-more-conservative-is-the-bmrs-approach">How much more conservative is the <code>bmrs</code>
approach?</a></li>
<li><a href="#directed-vs.-non-directed-hypothesis">Directed vs. non-directed
hypothesis</a></li>
<li><a href="#conclusion">Conclusion</a></li>
</ul>
<h1 id="introduction">Introduction</h1>
<p>In this post, I’d like to examine how different ways to calcualte Bayes
factors (BF) compare with each other for a simple model. This simulation
was run on my department’s HPC using the <code>rslurm</code> package: the
corresponding R code, data and .Rmd for this post are available.</p>
<p>For this simulation, I generated data of a within-subject model with two
levels. The simulation comprised 10,000 iteration per effect size
Cohen’s <em>d</em> (0.0, 0.5 and 0.8). Samples of 60 were generated with this
function:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">dataGenerator <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">function</span>(id_index, n, d){
  data_len <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">length</span>(id_index<span style="color:#f92672">:</span>(n <span style="color:#f92672">+</span> id_index <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>))
  x  <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#a6e22e">c</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>), data_len)
  b0 <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">rep</span>(<span style="color:#a6e22e">rnorm</span>(data_len), each <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>)
  y  <span style="color:#f92672">&lt;-</span> b0 <span style="color:#f92672">+</span> x <span style="color:#f92672">*</span> <span style="color:#a6e22e">rnorm</span>(data_len<span style="color:#f92672">*</span><span style="color:#ae81ff">2</span>, d)
  y  <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">scale</span>(y)
  
  df <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">data.frame</span>(id <span style="color:#f92672">=</span> <span style="color:#a6e22e">rep</span>(id_index<span style="color:#f92672">:</span>(n <span style="color:#f92672">+</span> id_index <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>), each <span style="color:#f92672">=</span> <span style="color:#ae81ff">2</span>),
                   x <span style="color:#f92672">=</span> x,
                   y <span style="color:#f92672">=</span> y)
  <span style="color:#a6e22e">return</span>(df)
}
</code></pre></div><p>The equation of this model can also be written  as:</p>
<p>$$y_{ji} = b_{0j} + x_{i}N(d, 1)$$</p>
<p>where $y_{ji}$ is the value for the j-th subject and the i-th level,
$b_{0j}$ a subject specific intercept and $x_{i}$ is the level
dummy-coded (0 &amp; 1) that is multiplied with a value drawn from normal
distribution with a mean of $d$ (0.0, 0.5 or 0.8) and a standard
deviation (SD) of 1. For the <code>brms</code> model, we scaled the data to a mean
of 0 and an SD of 1.</p>
<p>I then compared 4 ways to calculate a BF to examine the mean difference
of the two levels.</p>
<ol>
<li><code>ttestBF()</code>: For the first method, I used <code>ttestBF()</code> from the
<code>BayesFactor</code> package. This is a widely used Bayesian implementation
for the classical t-tests <a href="http://pcl.missouri.edu/sites/default/files/Rouder.bf_.pdf">(Rouder et
al., 2009)</a>.
For this simulation, I used the standard medium rscale of
<code>sqrt(2)/2</code> ≈ 0.707. Note that this approach uses different priors
(Cauchy/Jeffry’s prior) and likelihood functions than the <code>brms</code>
models below. So, I expect differences but it still can be used as a
reference point.</li>
</ol>
<p>The reaming four methods are all based on the same <code>brms</code>
model.</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">model_full  <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">brm</span>(y <span style="color:#f92672">~</span> x <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">|</span> id), data <span style="color:#f92672">=</span> df, prior <span style="color:#f92672">=</span> priors_normal_full)
</code></pre></div><p>For the intercept and the beta value, I placed a normal prior, N(0, 1):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">priors_normal_full <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">c</span>(<span style="color:#a6e22e">prior</span>(<span style="color:#a6e22e">normal</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>), class <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;Intercept&#34;</span>),
                        <span style="color:#a6e22e">prior</span>(<span style="color:#a6e22e">normal</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>), class <span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;b&#34;</span>))
</code></pre></div><ol start="2">
<li><code>bayes_factor()</code>: For this method, I compared the full model with a
null model that did not include x.</li>
</ol>
<p>The other two methods are based on an approximation of the BF that does
not calculate the marginal likelihood but instead the point density at
zero for the prior and the posterior distribution, which is known as the
<a href="https://www.ejwagenmakers.com/2010/WagenmakersEtAlCogPsy2010.pdf">Savage-Dickey
ratio</a>.</p>
<ol start="3">
<li>
<p><code>hypothesis()</code>: This function is a Savage-Dickey ratio
implementation from the <code>brms</code> package. Can be used like this
<code>hypothesis(model_full, 'x = 0')</code>.</p>
</li>
<li>
<p><strong>manual</strong>: This approach estimates the point density via Logspline
Density Estimation. This is adapted from code from Wagenmakers et
al. (2010) and can be found on <a href="https://www.ejwagenmakers.com/Code/2009/SavageDickeyPsychologists/SavageDickeyPsychologistsCode.zip">his
website</a>.
This method used <code>logspline()</code> from the <code>polspline</code> package. Note
that instead of also estimating the point density from the prior
samples as generated by <code>brms</code>, I just used the respective base
function <code>dnorm()</code>.</p>
</li>
</ol>
<!-- end list -->
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">fit.posterior  <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">logspline</span>(<span style="color:#a6e22e">posterior_samples</span>(model_full)<span style="color:#f92672">$</span>b_x)
posterior      <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">dlogspline</span>(<span style="color:#ae81ff">0</span>, fit.posterior) 
prior          <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">dnorm</span>(<span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">1</span>)
bf_manual      <span style="color:#f92672">&lt;-</span> prior<span style="color:#f92672">/</span>posterior
</code></pre></div><p>This method by the way also allows to easily calculate the BF for
directed hypotheses (i.e. d = 0 vs. d &gt;
0):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-r" data-lang="r">areaPosterior <span style="color:#f92672">&lt;-</span> <span style="color:#a6e22e">sum</span>(<span style="color:#a6e22e">posterior_samples</span>(model_full)<span style="color:#f92672">$</span>b_x <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0</span>)<span style="color:#f92672">/</span><span style="color:#a6e22e">length</span>(<span style="color:#a6e22e">posterior_samples</span>(model_full)<span style="color:#f92672">$</span>b_x)
posterior.OR  <span style="color:#f92672">&lt;-</span> posterior<span style="color:#f92672">/</span>areaPosterior    
prior.OR      <span style="color:#f92672">&lt;-</span> prior<span style="color:#f92672">/</span><span style="color:#ae81ff">0.5</span>
bf_manual_OR  <span style="color:#f92672">&lt;-</span> prior.OR<span style="color:#f92672">/</span>posterior.OR
</code></pre></div><p>See <a href="https://www.ejwagenmakers.com/2010/WagenmakersEtAlCogPsy2010.pdf">Wagenmakers et
al. (2010)</a>
for more information on this. This will also examined at the end of this
post.</p>
<h1 id="comparing-the-four-bf-methods">Comparing the four BF methods</h1>
<p>Below you find the scatterplots comparing the different ways of
estimating a BF. To aid illustration, the data is presented in natural
log-space. Each plot contains up to 10,000 data
points.</p>
<p><img src="/post/2020-07-16-comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode_files/figure-markdown_github/fig1-1.png" style="display: block; margin: auto;" /></p>
<p>The the scatterplots reveal when d = 0.0 that all methods compare well
with each other. In contrast for d = 0.5 and d = 0.8, <code>hypothesis()</code>
function starts to break down at around BF10 = exp(5) ≈ 150 and only
returns extremely inflated estimates. The manual method via Logspline
Density Estimation leads to more stable estimates, while the spread
(e.g. manual vs. <code>bayes_factor()</code>) also becomes larger at around BF10 =
exp(7) ≈ 1100. However, the estimations are not skewed as the ones
returned by <code>hypothesis()</code>. In that sense, the manual method based on
Logspline Density Estimation is superior. The reason for that is that
<code>hypothesis()</code> does not perform well if the estimation is only based on
very few values around zero, which happens when the distribution is
shifted away from zero.</p>
<p>As already noted above, there <code>ttestBF()</code> and the <code>brms</code> model have
different priors and likelihood functions, but comparison with the BF
from ttestBF is still useful to examine if they point in the same
general direction and indeed they do. In fact, the correlation for d =
0.5 and d = 0.8 is extremely linear. However, we also see that in the
top right corner that for d = 0.0 there is an interesting increasing
spread towards low values (e.g. see <code>ttestBF()</code> and <code>bayes_factor()</code>).
The reason for this spread reflect the different performance of the
methods to produce evidence for H0 (see next section).</p>
<p>All in all, the manual method does a good job with the advantage that it
is a lot after than <code>bayes_factor()</code> providing sensible estimates in
cases that matter most to use as psychologists/cognitive neurosciences.
In most circumstances, it doesn’t really matter if a BF10 is 1500 or
only 1100.</p>
<h1 id="evidence-for-null-hypothesis-brms-vs-ttestbf">Evidence for null hypothesis: <code>brms</code> vs. <code>ttestBF()</code></h1>
<p>To illustrate the difference between <code>brms</code> and <code>ttestBF()</code>, I binned
the BF into 8 categories and compared their
frequencies.</p>
<p><img src="/post/2020-07-16-comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode_files/figure-markdown_github/unnamed-chunk-7-1.png" alt=""><!-- --></p>
<p>A look at the two distributions shows that the <code>brms</code> model with unit
prior of N(0,1) leads to stronger evidence for the null when the null
hypothesis is true than <code>ttestBF()</code> with its Cauchy/Jeffrey’s prior
implementation does. Zero percent of <code>ttestBF()</code>’s BF10 were &lt; 1/10.
This highlights the more conservative nature of the <code>brms</code> model. As a
comparison, we can look at the distribution of BF10 when effect size is
d =
0.5.</p>
<p><img src="/post/2020-07-16-comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode_files/figure-markdown_github/unnamed-chunk-8-1.png" alt=""><!-- --></p>
<p>In this case the percentages are very similar despite being more
conservative, the <code>bmrs</code> model often leads to the same conclusion. Note
that for d = 0.8 all BF10 were above 10, hence they are not displayed
here but this illustrated that <code>brms</code> is doing well in these cases as
well.</p>
<h1 id="how-much-more-conservative-is-the-bmrs-approach">How much more conservative is the <code>bmrs</code> approach?</h1>
<p>The conservative nature can be illustrated by examining the distribution
of the ratios between both BFs
(<code>ttestBF()</code>/<code>bayes_factor()</code>).</p>
<p><img src="/post/2020-07-16-comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode_files/figure-markdown_github/unnamed-chunk-9-1.png" alt=""><!-- --></p>
<p>The distributions of ratios show that the <code>brms</code> model more conservative
than <code>ttestBF()</code> but as the effect size increase the max ratio drops
from 1.8 to 1.4. Median values are 1.308, 1.162 and 1.113 for a d of
0.0, 0.5 and 0.8, respectively. It can be seen that the unit-prior in
<code>brms</code> models regularises much more for lower d-values than <code>ttestBF()</code>,
which is a good thing if the aim is to provide evidence in favour of the
null hypothesis.</p>
<h1 id="directed-vs-non-directed-hypothesis">Directed vs. non-directed hypothesis</h1>
<p>On each run in these simulations, I also estimated the order-restricted
BF (i.e. BF for directed hypothesis) that d = 0 vs d &gt;
0.</p>
<p><img src="/post/2020-07-16-comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode_files/figure-markdown_github/unnamed-chunk-10-1.png" alt=""><!-- --></p>
<p>This shows that when the null is true, the order-restricted BF is much
lower (median of 0.989) compared to d = 0.8 because follows are uniform
distribution. For positive effect sizes (d = 0.5 &amp; d = 0.8), the ratio
is very close to 2.</p>
<h1 id="conclusion">Conclusion</h1>
<p>All in all, that simulation showed that the <code>brms</code> model with unit-prior
are more conservative than <code>ttestBF()</code> but lead to similar conclusions
if d &gt; 0. With respect to the four models, I would that the manual
implementation is usually enough as it and easy, quick and intuitive way
to calculate a BF. It definitely out performs the <code>hypothesis()</code>
function that easily breaks down if the posterior distribution has
little overlap with zero. Furthermore, <code>hypothesis()</code> does not work well
for priors that are hard to sample e.g. Cauchy prior. This is an issue
that has been <a href="https://rstudio-pubs-static.s3.amazonaws.com/358672_09291d0b37ce43f08cf001cfd25c16c2.html">previously
discussed</a>
but my tests show that the manual method gives quite reasonable
estimates in this scenario.</p>


    

    

    <h4>See also</h4>
    <ul>
        
            <li><a href="/2019/04/effect-of-sd-under-the-null-hypothesis/">Effect of SD under the null hypothesis</a></li>
        
            <li><a href="/2019/03/design-analysis-for-noveltyvr/">Design analysis for noveltyVR</a></li>
        
            <li><a href="/2019/03/prior-deliberation-on-analysis/">Prior deliberation on analysis</a></li>
        
    </ul>


</article>



        </div>

        <aside class="col-12 col-lg-3 ml-auto blog-sidebar">
    
        


<section>
    <h4>Recent Posts</h4>
    <ol class="list-unstyled">
        
        <li>
            <a href="/2020/07/comparing-different-methods-to-calculate-bayes-factors-for-a-simple-mode/">Comparing different methods to calculate Bayes factors for a simple mode</a>
        </li>
        
        <li>
            <a href="/post/">Posts</a>
        </li>
        
        <li>
            <a href="/2019/04/effect-of-sd-under-the-null-hypothesis/">Effect of SD under the null hypothesis</a>
        </li>
        
        <li>
            <a href="/2019/03/design-analysis-for-noveltyvr/">Design analysis for noveltyVR</a>
        </li>
        
        <li>
            <a href="/2019/03/prior-deliberation-on-analysis/">Prior deliberation on analysis</a>
        </li>
        
    </ol>
</section>

    
    
        <section>
    
        
    
        
        <h4>Categories</h4>
        <p>
            
            <a class="badge badge-primary" href="/categories/experiments">experiments</a>
            
            <a class="badge badge-primary" href="/categories/general">general</a>
            
        </p>
        
    
        
        <h4>Tags</h4>
        <p>
            
            <a class="badge badge-primary" href="/tags/about">about</a>
            
            <a class="badge badge-primary" href="/tags/bayesian-stats">bayesian-stats</a>
            
            <a class="badge badge-primary" href="/tags/data-check">data-check</a>
            
            <a class="badge badge-primary" href="/tags/design-analysis">design-analysis</a>
            
            <a class="badge badge-primary" href="/tags/familiarity">familiarity</a>
            
            <a class="badge badge-primary" href="/tags/glm">glm</a>
            
            <a class="badge badge-primary" href="/tags/graphics">graphics</a>
            
            <a class="badge badge-primary" href="/tags/hippocampus">hippocampus</a>
            
            <a class="badge badge-primary" href="/tags/ideas-and-plans">ideas-and-plans</a>
            
            <a class="badge badge-primary" href="/tags/lab-meeting">lab-meeting</a>
            
            <a class="badge badge-primary" href="/tags/lme4/lmertest">lme4/lmertest</a>
            
            <a class="badge badge-primary" href="/tags/memory">memory</a>
            
            <a class="badge badge-primary" href="/tags/meta">meta</a>
            
            <a class="badge badge-primary" href="/tags/mixed-linear-models">mixed-linear-models</a>
            
            <a class="badge badge-primary" href="/tags/mpfc">mpfc</a>
            
            <a class="badge badge-primary" href="/tags/noveltyvr">noveltyvr</a>
            
            <a class="badge badge-primary" href="/tags/pilot-study">pilot-study</a>
            
            <a class="badge badge-primary" href="/tags/ratingstudy">ratingstudy</a>
            
            <a class="badge badge-primary" href="/tags/recognition-memory">recognition-memory</a>
            
            <a class="badge badge-primary" href="/tags/recollection">recollection</a>
            
            <a class="badge badge-primary" href="/tags/research-design">research-design</a>
            
            <a class="badge badge-primary" href="/tags/rmarkdown">rmarkdown</a>
            
            <a class="badge badge-primary" href="/tags/schema">schema</a>
            
            <a class="badge badge-primary" href="/tags/schemavr">schemavr</a>
            
            <a class="badge badge-primary" href="/tags/schemavr1">schemavr1</a>
            
            <a class="badge badge-primary" href="/tags/schemavr2">schemavr2</a>
            
            <a class="badge badge-primary" href="/tags/unity3d">unity3d</a>
            
            <a class="badge badge-primary" href="/tags/word-properties">word-properties</a>
            
            <a class="badge badge-primary" href="/tags/words">words</a>
            
        </p>
        
    
</section>
    
</aside>

      </div>
    </div>
    

    
      






<footer class="blog-footer w-100">
    <nav class="navbar navbar-light bg-light">
        <p class="w-100 text-center">Hugo template made with ❤ by <a href="https://github.com/Xzya">Xzya</a>, inspired by <a href="https://github.com/alanorth/hugo-theme-bootstrap4-blog">hugo-theme-bootstrap4-blog</a></p>
        <p class="w-100 text-center"><a href="#">Back to top</a></p>
    </nav>
</footer>

<script src="//yihui.org/js/math-code.js"></script>
<script async
src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
</script>
 
    

    
    
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
  </body>
</html>
